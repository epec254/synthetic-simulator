{"request": [{"role": "system", "content": "You are a helpful AI assistant engaging in a conversation."}, {"role": "user", "content": "what is lakehouse monitoring?"}], "response": {"role": "assistant", "content": "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This monitoring is part of the broader management capabilities of the Databricks Lakehouse platform, which supports various workloads.\n\nKey features of lakehouse monitoring include:\n\n1. **Monitor Alerts**: You can create alerts based on metrics from monitor metrics tables. These alerts can notify you when certain statistics move out of a specified range, when data changes, or when data drifts compared to a baseline. Alerts can be configured to send notifications via email, webhooks, or applications like Slack or PagerDuty.\n\n2. **Data Quality Monitoring**: The system allows you to keep track of data quality across all tables, ensuring that data remains consistent and reliable.\n\n3. **Performance Tracking**: It includes tracking the performance of machine learning models and endpoints, helping you ensure that your models are functioning as expected.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)."}, "metadata": {"context_used_to_generate_question": null, "first_turn": true, "seed_question": "what is lakehouse monitoring?", "tag": "tool_outputs"}}
{"request": [{"role": "system", "content": "You are a helpful AI assistant engaging in a conversation."}, {"role": "user", "content": "what is lakehouse monitoring?"}, {"role": "assistant", "content": "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This monitoring is part of the broader management capabilities of the Databricks Lakehouse platform, which supports various workloads.\n\nKey features of lakehouse monitoring include:\n\n1. **Monitor Alerts**: You can create alerts based on metrics from monitor metrics tables. These alerts can notify you when certain statistics move out of a specified range, when data changes, or when data drifts compared to a baseline. Alerts can be configured to send notifications via email, webhooks, or applications like Slack or PagerDuty.\n\n2. **Data Quality Monitoring**: The system allows you to keep track of data quality across all tables, ensuring that data remains consistent and reliable.\n\n3. **Performance Tracking**: It includes tracking the performance of machine learning models and endpoints, helping you ensure that your models are functioning as expected.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)."}, {"role": "user", "content": "What are some common uses for monitor alerts in Databricks Lakehouse Monitoring?"}], "response": {"role": "assistant", "content": "In Databricks Lakehouse Monitoring, monitor alerts are used to proactively manage and maintain the quality and performance of data and machine learning models. Some common uses for monitor alerts include:\n\n1. **Data Quality Alerts**: \n   - Detect anomalies or unexpected changes in data patterns.\n   - Monitor for missing or null values in critical data fields.\n   - Track data freshness to ensure data is updated within expected timeframes.\n\n2. **Data Drift Alerts**:\n   - Identify when the distribution of incoming data deviates significantly from historical data, which might indicate changes in data sources or processes.\n\n3. **Model Performance Alerts**:\n   - Monitor key performance metrics of machine learning models, such as accuracy, precision, recall, or F1 score.\n   - Alert when model performance degrades beyond acceptable thresholds, indicating potential issues with model inputs or changes in underlying data.\n\n4. **Resource Utilization Alerts**:\n   - Track the usage of computational resources to prevent overuse or underutilization.\n   - Alert when resource consumption exceeds predefined limits, which can help in optimizing costs and performance.\n\n5. **Operational Alerts**:\n   - Notify when scheduled jobs or workflows fail or take longer than expected to complete.\n   - Monitor the status of data pipelines to ensure they are running smoothly and efficiently.\n\n6. **Security and Compliance Alerts**:\n   - Detect unauthorized access attempts or unusual activity patterns that could indicate security breaches.\n   - Ensure compliance with data governance policies by monitoring access and usage patterns.\n\nThese alerts help organizations maintain high data quality, ensure model reliability, optimize resource usage, and uphold security and compliance standards."}, "metadata": {"context_used_to_generate_question": "[{\"page_content\": \"# Introduction to Databricks Lakehouse Monitoring\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html\", \"score\": 2, \"chunk_id\": \"46a5182ce48b20d7617a750390b29baa\"}}, {\"page_content\": \"# Introduction to Databricks Lakehouse Monitoring\\n### Monitor alerts\\n\\nPreview  \\nThis feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).  \\nThis page describes how to create a Databricks SQL alert based on a metric from a monitor metrics table. Some common uses for monitor alerts include:  \\n* Get notified when a statistic moves out of a certain range. For example, you want to receive a notification when the fraction of missing values exceeds a certain level.\\n* Get notified of a change in the data. The drift metrics table stores statistics that track changes in the data distribution.\\n* Get notified if data has drifted in comparison to the baseline table. You can set up an alert to investigate the data changes or, for `InferenceLog` analysis, to indicate that the model should be retrained.  \\nMonitor alerts are created and used the same way as other Databricks SQL alerts. You create a [Databricks SQL query](https://docs.databricks.com/sql/user/queries/index.html) on the monitor profile metrics table or drift metrics table. You then create a Databricks SQL alert for this query. You can configure the alert to evaluate the query at a desired frequency, and send a notification if the alert is triggered. By default, email notification is sent. You can also set up a webhook or send notifications to other applications such as Slack or Pagerduty.  \\nYou can also quickly create an alert from the [monitor dashboard](https://docs.databricks.com/lakehouse-monitoring/monitor-dashboard.html) as follows:  \\n1. On the dashboard, find the chart for which you want to create an alert.\\n2. Click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) in the upper-right corner of the chart and select **View query**. The SQL editor opens.\\n3. In the SQL editor, click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) above the editor window and select **Create alert**. The **New alert** dialog opens in a new tab.\\n4. Configure the alert and click **Create alert**.  \\nNote that if the query uses parameters, then the alert is based on the default values for these parameters. You should confirm that the default values reflect the intent of the alert.  \\nFor details, see [Databricks SQL alerts](https://docs.databricks.com/sql/user/alerts/index.html).\\n\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html\", \"score\": 2, \"chunk_id\": \"ad46dd649af7864f934123978c11d492\"}}, {\"page_content\": \"# Introduction to the well-architected data lakehouse\\n### Download lakehouse reference architectures\\n#### Capabilities for your workloads\\n\\nIn addition, the Databricks lakehouse comes with management capabilities that support all workloads:  \\n* **Data and AI governance**  \\nThe central data and AI governance system in the Databricks Data Intelligence Platform is [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html). Unity Catalog provides a single place to manage data access policies that apply across all workspaces and supports all assets created or used in the lakehouse, such as tables, volumes, features ([feature store](https://docs.databricks.com/machine-learning/feature-store/index.html)), and models ([model registry](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)). Unity Catalog can also be used to [capture runtime data lineage](https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html) across queries run on Databricks.  \\nDatabricks [lakehouse monitoring](https://docs.databricks.com/lakehouse-monitoring/index.html) allows you to monitor the quality of the data in all of the tables in your account. It can also track the performance of [machine learning models and model-serving endpoints](https://docs.databricks.com/machine-learning/model-serving/monitor-diagnose-endpoints.html).  \\nFor Observability, [system tables](https://docs.databricks.com/admin/system-tables/index.html) is a Databricks-hosted analytical store of your account\\u2019s operational data. System tables can be used for historical observability across your account.\\n* **Data intelligence engine**  \\nThe Databricks Data Intelligence Platform allows your entire organization to use data and AI. It is powered by [DatabricksIQ](https://docs.databricks.com/databricksiq/index.html) and combines generative AI with the unification benefits of a lakehouse to understand the unique semantics of your data.  \\nThe [Databricks Assistant](https://docs.databricks.com/notebooks/databricks-assistant-faq.html) is available in Databricks notebooks, SQL editor, and file editor as a context-aware AI assistant for developers.  \\n* **Orchestration**  \\n[Databricks Workflows](https://docs.databricks.com/workflows/index.html) orchestrate data processing, machine learning, and analytics pipelines in the Databricks Data Intelligence Platform. Workflows has fully managed orchestration services integrated into the Databricks platform, including [Databricks Jobs](https://docs.databricks.com/workflows/index.html#what-is-databricks-jobs) to run non-interactive code in your Databricks workspace and [Delta Live Tables](https://docs.databricks.com/delta-live-tables/index.html) to build reliable and maintainable ETL pipelines.\\n\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-architecture/reference.html\", \"score\": 2, \"chunk_id\": \"95c2d07889b26b243d43b632ba0fe699\"}}, {\"page_content\": \"# Databricks release notes\\n### Databricks platform release notes\\n\\n* [May 2024](https://docs.databricks.com/release-notes/product/2024/may.html)\\n+ [Compute plane outbound IP addresses must be added to a workspace IP allow list](https://docs.databricks.com/release-notes/product/2024/may.html#compute-plane-outbound-ip-addresses-must-be-added-to-a-workspace-ip-allow-list)\\n+ [OAuth is supported in Lakehouse Federation for Snowflake](https://docs.databricks.com/release-notes/product/2024/may.html#oauth-is-supported-in-lakehouse-federation-for-snowflake)\\n+ [Bulk move and delete workspace objects from the workspace browser](https://docs.databricks.com/release-notes/product/2024/may.html#bulk-move-and-delete-workspace-objects-from-the-workspace-browser)\\n+ [New compliance and security settings APIs (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#new-compliance-and-security-settings-apis-public-preview)\\n+ [Databricks Runtime 15.2 is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-is-ga)\\n+ [New Tableau connector for Delta Sharing](https://docs.databricks.com/release-notes/product/2024/may.html#new-tableau-connector-for-delta-sharing)\\n+ [New deep learning recommendation model examples](https://docs.databricks.com/release-notes/product/2024/may.html#new-deep-learning-recommendation-model-examples)\\n+ [Bind storage credentials and external locations to specific workspaces (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#bind-storage-credentials-and-external-locations-to-specific-workspaces-public-preview)\\n+ [Git folders are GA](https://docs.databricks.com/release-notes/product/2024/may.html#git-folders-are-ga)\\n+ [Pre-trained models in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#pre-trained-models-in-unity-catalog-public-preview)\\n+ [Databricks Vector Search is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-vector-search-is-ga)\\n+ [The compliance security profile now supports AWS Graviton instance types](https://docs.databricks.com/release-notes/product/2024/may.html#the-compliance-security-profile-now-supports-aws-graviton-instance-types)\\n+ [Databricks Assistant autocomplete (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-assistant-autocomplete-public-preview)\\n+ [Meta Llama 3 support in Foundation Model Training](https://docs.databricks.com/release-notes/product/2024/may.html#meta-llama-3-support-in-foundation-model-training)\\n+ [New changes to Git folder UI](https://docs.databricks.com/release-notes/product/2024/may.html#new-changes-to-git-folder-ui)\\n+ [Compute now uses EBS GP3 volumes for autoscaling local storage](https://docs.databricks.com/release-notes/product/2024/may.html#compute-now-uses-ebs-gp3-volumes-for-autoscaling-local-storage)\\n+ [Unified Login now supported with AWS PrivateLink (Private Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#unified-login-now-supported-with-aws-privatelink-private-preview)\\n+ [Foundation Model Training (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#foundation-model-training-public-preview)\\n+ [Attribute tag values for Unity Catalog objects can now be 1000 characters long (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#attribute-tag-values-for-unity-catalog-objects-can-now-be-1000-characters-long-public-preview)\\n+ [New Previews page](https://docs.databricks.com/release-notes/product/2024/may.html#new-previews-page)\\n+ [New capabilities for Databricks Vector Search](https://docs.databricks.com/release-notes/product/2024/may.html#new-capabilities-for-databricks-vector-search)\\n+ [Credential passthrough and Hive metastore table access controls are deprecated](https://docs.databricks.com/release-notes/product/2024/may.html#credential-passthrough-and-hive-metastore-table-access-controls-are-deprecated)\\n+ [Databricks JDBC driver 2.6.38](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-jdbc-driver-2638)\\n+ [Databricks Runtime 15.2 (Beta)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-beta)\\n+ [Notebooks now detect and auto-complete column names for Spark Connect DataFrames](https://docs.databricks.com/release-notes/product/2024/may.html#notebooks-now-detect-and-auto-complete-column-names-for-spark-connect-dataframes)\\n* [April 2024](https://docs.databricks.com/release-notes/product/2024/april.html)\\n+ [Databricks Runtime 15.1 is GA](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-runtime-151-is-ga)\\n+ [Databricks Assistant: Threads & history](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-assistant-threads--history)\\n+ [Cancel pending serving endpoint updates in Model Serving](https://docs.databricks.com/release-notes/product/2024/april.html#cancel-pending-serving-endpoint-updates-in-model-serving)\\n+ [Data lineage now captures reads on tables with column masks and row-level security](https://docs.databricks.com/release-notes/product/2024/april.html#data-lineage-now-captures-reads-on-tables-with-column-masks-and-row-level-security)\\n+ [Meta Llama 3 is supported in Model Serving for AWS](https://docs.databricks.com/release-notes/product/2024/april.html#meta-llama-3-is-supported-in-model-serving-for-aws)\\n+ [Notebooks now automatically detect SQL](https://docs.databricks.com/release-notes/product/2024/april.html#notebooks-now-automatically-detect-sql)\\n+ [New columns added to the billable usage system table (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#new-columns-added-to-the-billable-usage-system-table-public-preview)\\n+ [Delta Sharing supports tables that use column mapping (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-sharing-supports-tables-that-use-column-mapping-public-preview)\\n+ [Get serving endpoint schemas (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#get-serving-endpoint-schemas-public-preview)\\n+ [Creation and installation of workspace libraries is no longer available](https://docs.databricks.com/release-notes/product/2024/april.html#creation-and-installation-of-workspace-libraries-is-no-longer-available)\\n+ [Jobs created through the UI are now queued by default](https://docs.databricks.com/release-notes/product/2024/april.html#jobs-created-through-the-ui-are-now-queued-by-default)\\n+ [Configuring access to resources from serving endpoints is GA](https://docs.databricks.com/release-notes/product/2024/april.html#configuring-access-to-resources-from-serving-endpoints-is-ga)\\n+ [Serverless compute for workflows is in public preview](https://docs.databricks.com/release-notes/product/2024/april.html#serverless-compute-for-workflows-is-in-public-preview)\\n+ [Lakehouse Federation supports foreign tables with case-sensitive identifiers](https://docs.databricks.com/release-notes/product/2024/april.html#lakehouse-federation-supports-foreign-tables-with-case-sensitive-identifiers)\\n+ [Compute cloning now clones any libraries installed on the original compute](https://docs.databricks.com/release-notes/product/2024/april.html#compute-cloning-now-clones-any-libraries-installed-on-the-original-compute)\\n+ [Route optimization is available for serving endpoints](https://docs.databricks.com/release-notes/product/2024/april.html#route-optimization-is-available-for-serving-endpoints)\\n+ [Delta Live Tables notebook developer experience improvements (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-live-tables-notebook-developer-experience-improvements-public-preview)\\n+ [Databricks on AWS GovCloud (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-on-aws-govcloud-public-preview)\\n* [March 2024](https://docs.databricks.com/release-notes/product/2024/march.html)\\n+ [DBRX Base and DBRX Instruct are now available in Model Serving](https://docs.databricks.com/release-notes/product/2024/march.html#dbrx-base-and-dbrx-instruct-are-now-available-in-model-serving)\\n+ [Model Serving is HIPAA compliant in all regions](https://docs.databricks.com/release-notes/product/2024/march.html#model-serving-is-hipaa-compliant-in-all-regions)\\n+ [Provisioned throughput in Foundation Model APIs is GA and HIPAA compliant](https://docs.databricks.com/release-notes/product/2024/march.html#provisioned-throughput-in-foundation-model-apis-is-ga-and-hipaa-compliant)\\n+ [MLflow now enforces quota limits for experiments and runs](https://docs.databricks.com/release-notes/product/2024/march.html#mlflow-now-enforces-quota-limits-for-experiments-and-runs)\\n+ [The Jobs UI is updated to better manage jobs deployed by Databricks Asset Bundles](https://docs.databricks.com/release-notes/product/2024/march.html#the-jobs-ui-is-updated-to-better-manage-jobs-deployed-by-databricks-asset-bundles)\\n+ [Google Cloud Vertex AI supported as model provider for external models](https://docs.databricks.com/release-notes/product/2024/march.html#google-cloud-vertex-ai-supported-as-model-provider-for-external-models)\\n+ [Access resources from serving endpoints using instance profiles is GA](https://docs.databricks.com/release-notes/product/2024/march.html#access-resources-from-serving-endpoints-using-instance-profiles-is-ga)\\n+ [Interactive notebook debugging](https://docs.databricks.com/release-notes/product/2024/march.html#interactive-notebook-debugging)\\n+ [Self-service sign-up for private exchange providers in Marketplace](https://docs.databricks.com/release-notes/product/2024/march.html#self-service-sign-up-for-private-exchange-providers-in-marketplace)\\n+ [Databricks Runtime 15.0 is GA](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-is-ga)\\n+ [Databricks Repos changed to Git folders](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-repos-changed-to-git-folders)\\n+ [Databricks Runtime 14.1 and 14.2 series support extended](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-141-and-142-series-support-extended)\\n+ [Databricks ODBC driver 2.8.0](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-odbc-driver-280)\\n+ [SQL warehouses for notebooks is GA](https://docs.databricks.com/release-notes/product/2024/march.html#sql-warehouses-for-notebooks-is-ga)\\n+ [Delegate the ability to view an object\\u2019s metadata in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/march.html#delegate-the-ability-to-view-an-objects-metadata-in-unity-catalog-public-preview)\\n+ [Databricks Runtime 15.0 (Beta)](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-beta)\\n+ [Databricks Runtime 14.0 series support ends](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-140-series-support-ends)\\n+ [New computation for sys.path and CWD in Repos](https://docs.databricks.com/release-notes/product/2024/march.html#new-computation-for-syspath-and-cwd-in-repos)\\n+ [Fe\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/release-notes/product/index.html\", \"score\": 2, \"chunk_id\": \"3c7d1aad122c45b35c16a852e6cc35b5\"}}]", "first_turn": false, "tag": "tool_outputs"}}
{"request": [{"role": "system", "content": "You are a helpful AI assistant engaging in a conversation."}, {"role": "user", "content": "what is lakehouse monitoring?"}], "response": {"role": "assistant", "content": "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This monitoring is part of the broader management capabilities of the Databricks Lakehouse platform, which supports various workloads.\n\nKey features of lakehouse monitoring include:\n\n1. **Monitor Alerts**: You can create alerts based on metrics from monitor metrics tables. These alerts can notify you when certain statistics move out of a specified range, when data changes, or when data drifts compared to a baseline. Alerts can be configured to send notifications via email, webhooks, or applications like Slack or PagerDuty.\n\n2. **Data Quality Monitoring**: The system allows you to keep track of data quality across your tables, ensuring that your data remains reliable and accurate.\n\n3. **Performance Tracking**: It also includes tracking the performance of machine learning models and endpoints, helping you ensure that your models are functioning as expected.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)."}, "metadata": {"context_used_to_generate_question": null, "first_turn": true, "seed_question": "what is lakehouse monitoring?", "tag": "agent_response"}}
{"request": [{"role": "system", "content": "You are a helpful AI assistant engaging in a conversation."}, {"role": "user", "content": "what is lakehouse monitoring?"}, {"role": "assistant", "content": "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This monitoring is part of the broader management capabilities of the Databricks Lakehouse platform, which supports various workloads.\n\nKey features of lakehouse monitoring include:\n\n1. **Monitor Alerts**: You can create alerts based on metrics from monitor metrics tables. These alerts can notify you when certain statistics move out of a specified range, when data changes, or when data drifts compared to a baseline. Alerts can be configured to send notifications via email, webhooks, or applications like Slack or PagerDuty.\n\n2. **Data Quality Monitoring**: The system allows you to keep track of data quality across your tables, ensuring that your data remains reliable and accurate.\n\n3. **Performance Tracking**: It also includes tracking the performance of machine learning models and endpoints, helping you ensure that your models are functioning as expected.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)."}, {"role": "user", "content": "How can alerts be configured in lakehouse monitoring?"}], "response": {"role": "assistant", "content": "I couldn't find specific documentation on configuring alerts in lakehouse monitoring. However, generally, alerts in Databricks Lakehouse Monitoring can be configured to notify you when certain metrics or conditions are met. This typically involves setting up rules or thresholds for specific metrics, and then specifying how you want to be notified (e.g., via email, Slack, or other integrations).\n\nFor detailed steps and options, I recommend checking the official Databricks documentation or reaching out to Databricks support for guidance on setting up alerts specific to your use case."}, "metadata": {"context_used_to_generate_question": "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This monitoring is part of the broader management capabilities of the Databricks Lakehouse platform, which supports various workloads.\n\nKey features of lakehouse monitoring include:\n\n1. **Monitor Alerts**: You can create alerts based on metrics from monitor metrics tables. These alerts can notify you when certain statistics move out of a specified range, when data changes, or when data drifts compared to a baseline. Alerts can be configured to send notifications via email, webhooks, or applications like Slack or PagerDuty.\n\n2. **Data Quality Monitoring**: The system allows you to keep track of data quality across your tables, ensuring that your data remains reliable and accurate.\n\n3. **Performance Tracking**: It also includes tracking the performance of machine learning models and endpoints, helping you ensure that your models are functioning as expected.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).", "first_turn": false, "tag": "agent_response"}}
{"request": [{"role": "system", "content": "You are a helpful AI assistant engaging in a conversation."}, {"role": "user", "content": "what is lakehouse monitoring?"}, {"role": "assistant", "content": "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This monitoring is part of the broader management capabilities of the Databricks Lakehouse platform, which supports various workloads.\n\nKey features of lakehouse monitoring include:\n\n1. **Monitor Alerts**: You can create alerts based on metrics from monitor metrics tables. These alerts can notify you when certain statistics move out of a specified range, when data changes, or when data drifts compared to a baseline. Alerts can be configured to send notifications via email, webhooks, or applications like Slack or PagerDuty.\n\n2. **Data Quality Monitoring**: The system allows you to keep track of data quality across your tables, ensuring that your data remains reliable and accurate.\n\n3. **Performance Tracking**: It also includes tracking the performance of machine learning models and endpoints, helping you ensure that your models are functioning as expected.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)."}, {"role": "user", "content": "How can alerts be configured in lakehouse monitoring?"}, {"role": "assistant", "content": "I couldn't find specific documentation on configuring alerts in lakehouse monitoring. However, generally, alerts in Databricks Lakehouse Monitoring can be configured to notify you when certain metrics or conditions are met. This typically involves setting up rules or thresholds for specific metrics, and then specifying how you want to be notified (e.g., via email, Slack, or other integrations).\n\nFor detailed steps and options, I recommend checking the official Databricks documentation or reaching out to Databricks support for guidance on setting up alerts specific to your use case."}, {"role": "user", "content": "How can alerts notify you in Lakehouse Monitoring?"}], "response": {"role": "assistant", "content": "In Databricks Lakehouse Monitoring, alerts can notify you through several methods:\n\n1. **Email Notifications**: By default, alerts send notifications via email when triggered.\n\n2. **Webhooks**: You can configure alerts to send notifications through webhooks, allowing integration with various services and applications.\n\n3. **Third-party Applications**: Alerts can also be set up to send notifications to applications like Slack or PagerDuty, enabling real-time communication and incident management.\n\nTo create an alert, you typically set up a Databricks SQL query on the monitor profile metrics table or drift metrics table, and then create a Databricks SQL alert for this query. You can configure the alert to evaluate the query at a desired frequency and specify the notification method.\n\nFor more detailed steps on setting up alerts, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)."}, "metadata": {"context_used_to_generate_question": "I couldn't find specific documentation on configuring alerts in lakehouse monitoring. However, generally, alerts in Databricks Lakehouse Monitoring can be configured to notify you when certain metrics or conditions are met. This typically involves setting up rules or thresholds for specific metrics, and then specifying how you want to be notified (e.g., via email, Slack, or other integrations).\n\nFor detailed steps and options, I recommend checking the official Databricks documentation or reaching out to Databricks support for guidance on setting up alerts specific to your use case.", "first_turn": false, "tag": "agent_response"}}
