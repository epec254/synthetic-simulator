{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029eb0ad-63fc-4faa-a202-c67feb3e04c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqqq \"requests>=2.32.3\", \"databricks-agents>=0.13.0\", \"databricks-sdk[openai]>=0.40.0\", \"backoff>=2.2.1\", \"mlflow\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edcac892-70b8-4c80-8b3c-14a63402904f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import simulator_service\n",
    "import logging\n",
    "from typing import Callable, Dict, Any, List\n",
    "from simulator_service import SyntheticDataSimulatorService\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "from simulator_service.mlflow_utils import invoke_model_with_trace\n",
    "from simulator_service.context_generators import (\n",
    "    get_all_tool_outputs_from_agent_trace,\n",
    "    get_agent_response_from_trace,\n",
    ")\n",
    "from simulator_service.synthetic_generation import (\n",
    "    generate_next_question_using_context_from_previous_turn,\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "logger = logging.getLogger(\"simulator_service.examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478c761b-d45a-4196-b1c5-2dcf98fc051d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d71a5f-80ed-42b7-9ebb-9b0264ed4cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agent MLflow logging helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8643bb85-3fb8-4952-8d3b-cf466f4ae43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def log_model(\n",
    "    agent_code_file: str, agent_config: dict\n",
    ") -> mlflow.models.model.ModelInfo:\n",
    "    \"\"\"Log the model to MLflow and return the model info.\n",
    "\n",
    "    Args:\n",
    "        agent_code_file: Path to the agent code file\n",
    "        agent_config: Configuration dictionary for the agent\n",
    "\n",
    "    Returns:\n",
    "        ModelInfo: Information about the logged model\n",
    "    \"\"\"\n",
    "    return mlflow.pyfunc.log_model(\n",
    "        python_model=agent_code_file,\n",
    "        artifact_path=\"agent\",\n",
    "        model_config=agent_config,\n",
    "        resources=[\n",
    "            DatabricksServingEndpoint(endpoint_name=agent_config[\"endpoint_name\"])\n",
    "        ],\n",
    "        input_example={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"What is lakehouse monitoring?\"}]\n",
    "        },\n",
    "        pip_requirements=[\n",
    "            \"databricks-sdk[openai]\",\n",
    "            \"mlflow\",\n",
    "            \"databricks-agents\",\n",
    "            \"backoff\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827f7e50-9dc4-4af3-9fb2-349674f994f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wrapper function so the synthetic service can call the logged Agent\n",
    "\n",
    "See: https://github.com/epec254/synthetic-simulator?tab=readme-ov-file#required-callable-signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d976e0d0-d1f5-4f28-b625-91461c6f0bcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_agent_callable(\n",
    "    model_info: mlflow.models.model.ModelInfo,\n",
    ") -> Callable[[List[Dict[str, str]]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build a chat completion function that uses a logged MLflow model.\n",
    "    This is passed to `SyntheticDataSimulatorService(chat_agent_callable=...)`\n",
    "\n",
    "    Args:\n",
    "        model_info: ModelInfo object returned from log_model()\n",
    "\n",
    "    Returns:\n",
    "        Callable that implements chat completion API using the logged model\n",
    "    \"\"\"\n",
    "    # Load the model once when creating the completion function\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "    def call_mlflow_logged_agent(messages: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "        # print(messages)\n",
    "        # Use invoke_model_uri_with_trace with the cached model's URI\n",
    "        outputs, output_trace = invoke_model_with_trace(\n",
    "            model=loaded_model,\n",
    "            model_input={\"messages\": messages},\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"outputs\": outputs,\n",
    "            \"output_trace\": output_trace,\n",
    "        }\n",
    "\n",
    "    return call_mlflow_logged_agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0b0b13-2987-4edc-94c7-e2428fb28ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Synthetic generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f964c45-9614-4b61-ac37-72a99ae882ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Approach #1: Use the agent's tool outputs to synthesize the next turn of conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b28c9a2-164e-4442-ae75-a5621a38fc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_based_on_tool_outputs(\n",
    "    max_turns: int,\n",
    "    model_info: mlflow.models.model.ModelInfo,\n",
    "    output_file: str,\n",
    "    seed_question: str,\n",
    "    agent_description: str,\n",
    "    tag: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Synthetically generates follow-up questions based on the outputs from all called tools.\n",
    "\n",
    "    Args:\n",
    "        max_turns: Maximum number of conversation turns\n",
    "        model_info: ModelInfo object returned from log_model()\n",
    "        output_file: Path to the output file for conversation history\n",
    "        seed_question: Initial question to start the conversation\n",
    "        agent_description: Description of the chat agent\n",
    "        tag: Tag for the generation type\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\n",
    "        f\"Starting synthetic generation with {max_turns} turns based on tool outputs\"\n",
    "    )\n",
    "\n",
    "    chat_completion_callable = get_agent_callable(model_info)\n",
    "\n",
    "    simulator_service = SyntheticDataSimulatorService(\n",
    "        chat_agent_callable=chat_completion_callable,\n",
    "        question_generator_callable=generate_next_question_using_context_from_previous_turn,\n",
    "        get_context_from_chat_agent_response_for_next_turn_callable=get_all_tool_outputs_from_agent_trace,\n",
    "        max_turns=max_turns,\n",
    "        seed_question=seed_question,\n",
    "        output_file=output_file,\n",
    "        agent_description=agent_description,\n",
    "        tag=tag,\n",
    "    )\n",
    "\n",
    "    # Start the conversation\n",
    "    try:\n",
    "        simulator_service.start_conversation()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during conversation: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81569cec-cf4f-4efa-a3ca-7e52735250c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Approach #2: Use the agent's response to synthesize the next turn of conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81710f79-69b4-4298-bd1c-12ab81db7053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_based_on_response(\n",
    "    max_turns: int,\n",
    "    model_info: mlflow.models.model.ModelInfo,\n",
    "    output_file: str,\n",
    "    seed_question: str,\n",
    "    agent_description: str,\n",
    "    tag: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Synthetically generates follow-up questions based on the agent's last response.\n",
    "\n",
    "    Args:\n",
    "        max_turns: Maximum number of conversation turns\n",
    "        model_info: ModelInfo object returned from log_model()\n",
    "        output_file: Path to the output file for conversation history\n",
    "        seed_question: Initial question to start the conversation\n",
    "        agent_description: Description of the chat agent\n",
    "        tag: Tag for the generation type\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\n",
    "        f\"Starting synthetic generation with {max_turns} turns based on response content\"\n",
    "    )\n",
    "\n",
    "    chat_completion_callable = get_agent_callable(model_info)\n",
    "\n",
    "    simulator_service = SyntheticDataSimulatorService(\n",
    "        chat_agent_callable=chat_completion_callable,\n",
    "        question_generator_callable=generate_next_question_using_context_from_previous_turn,\n",
    "        get_context_from_chat_agent_response_for_next_turn_callable=get_agent_response_from_trace,\n",
    "        max_turns=max_turns,\n",
    "        seed_question=seed_question,\n",
    "        output_file=output_file,\n",
    "        agent_description=agent_description,\n",
    "        tag=tag,\n",
    "    )\n",
    "\n",
    "    # Start the conversation\n",
    "    try:\n",
    "        simulator_service.start_conversation()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during conversation: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0900562d-5d24-4ee6-a6e7-8c4fef200c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476b66e0-2c74-4105-b04c-6e11451e4634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/13 15:12:46 INFO mlflow.pyfunc: Predicting on input example to validate output\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8a9e1934a549c5970f588a218d5320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5429957f74bf4d958123ae087fcd9e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:13:09,411 - chat_service - INFO - Initial turn: Asking seed question: what is lakehouse monitoring?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Initial turn: Asking seed question: what is lakehouse monitoring?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:13:20,413 - chat_service - INFO - Answer: Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This includes setting up alerts to notify you when certain metrics or statistics move out of a specified range, indicating potential issues such as data drift or the need for model retraining.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Answer: Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This includes setting up alerts to notify you when certain metrics or statistics move out of a specified range, indicating potential issues such as data drift or the need for model retraining.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:13:29,576 - chat_service - INFO - Turn 1: Asking question: How can I receive notifications for Databricks Lakehouse monitor alerts?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Turn 1: Asking question: How can I receive notifications for Databricks Lakehouse monitor alerts?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:13:38,523 - chat_service - INFO - Answer: To receive notifications for Databricks Lakehouse monitor alerts, you can set up alerts using Databricks SQL. Here's how you can do it:\n\n1. **Create a Databricks SQL Query**: First, create a SQL query on the monitor profile metrics table or drift metrics table. This query will define the conditions under which you want to be alerted.\n\n2. **Create a Databricks SQL Alert**: Once you have your query, create an alert for it. You can configure the alert to evaluate the query at a desired frequency.\n\n3. **Configure Notifications**: By default, email notifications are sent when an alert is triggered. However, you can also set up notifications to be sent to other applications such as Slack or PagerDuty using webhooks.\n\n4. **Quick Alert Creation from Dashboard**: You can also create an alert directly from the monitor dashboard:\n   - Find the chart for which you want to create an alert.\n   - Click the kebab menu (three vertical dots) in the upper-right corner of the chart and select \"View query.\"\n   - In the SQL editor, click the kebab menu above the editor window and select \"Create alert.\"\n   - Configure the alert and click \"Create alert.\"\n\nFor more detailed steps and options, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Answer: To receive notifications for Databricks Lakehouse monitor alerts, you can set up alerts using Databricks SQL. Here's how you can do it:\n\n1. **Create a Databricks SQL Query**: First, create a SQL query on the monitor profile metrics table or drift metrics table. This query will define the conditions under which you want to be alerted.\n\n2. **Create a Databricks SQL Alert**: Once you have your query, create an alert for it. You can configure the alert to evaluate the query at a desired frequency.\n\n3. **Configure Notifications**: By default, email notifications are sent when an alert is triggered. However, you can also set up notifications to be sent to other applications such as Slack or PagerDuty using webhooks.\n\n4. **Quick Alert Creation from Dashboard**: You can also create an alert directly from the monitor dashboard:\n   - Find the chart for which you want to create an alert.\n   - Click the kebab menu (three vertical dots) in the upper-right corner of the chart and select \"View query.\"\n   - In the SQL editor, click the kebab menu above the editor window and select \"Create alert.\"\n   - Configure the alert and click \"Create alert.\"\n\nFor more detailed steps and options, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:13:44,140 - chat_service - INFO - Turn 2: Asking question: How do I create an alert from the monitor dashboard?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Turn 2: Asking question: How do I create an alert from the monitor dashboard?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:13:50,612 - chat_service - INFO - Answer: To create an alert from the monitor dashboard in Databricks, follow these steps:\n\n1. **Access the Monitor Dashboard**: Navigate to the monitor dashboard where you have the visualizations of your data metrics.\n\n2. **Select the Chart**: Identify the chart for which you want to create an alert. This chart should represent the data or metric you want to monitor.\n\n3. **Open the Chart Menu**: Click on the kebab menu (three vertical dots) located in the upper-right corner of the chart.\n\n4. **View the Query**: From the dropdown menu, select \"View query.\" This action will open the SQL editor with the query used to generate the chart.\n\n5. **Create the Alert**: In the SQL editor, locate the kebab menu above the editor window. Click on it and select \"Create alert.\"\n\n6. **Configure the Alert**: You will be prompted to configure the alert settings. This includes setting the conditions under which the alert should trigger, the frequency of evaluation, and the notification settings.\n\n7. **Finalize the Alert**: After configuring the alert, click \"Create alert\" to finalize and activate it.\n\nBy following these steps, you can easily set up alerts directly from the monitor dashboard, ensuring you receive timely notifications based on your data metrics.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Answer: To create an alert from the monitor dashboard in Databricks, follow these steps:\n\n1. **Access the Monitor Dashboard**: Navigate to the monitor dashboard where you have the visualizations of your data metrics.\n\n2. **Select the Chart**: Identify the chart for which you want to create an alert. This chart should represent the data or metric you want to monitor.\n\n3. **Open the Chart Menu**: Click on the kebab menu (three vertical dots) located in the upper-right corner of the chart.\n\n4. **View the Query**: From the dropdown menu, select \"View query.\" This action will open the SQL editor with the query used to generate the chart.\n\n5. **Create the Alert**: In the SQL editor, locate the kebab menu above the editor window. Click on it and select \"Create alert.\"\n\n6. **Configure the Alert**: You will be prompted to configure the alert settings. This includes setting the conditions under which the alert should trigger, the frequency of evaluation, and the notification settings.\n\n7. **Finalize the Alert**: After configuring the alert, click \"Create alert\" to finalize and activate it.\n\nBy following these steps, you can easily set up alerts directly from the monitor dashboard, ensuring you receive timely notifications based on your data metrics.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99835c486164da29387c7307308c318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:13:54,097 - chat_service - INFO - Initial turn: Asking seed question: what is lakehouse monitoring?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Initial turn: Asking seed question: what is lakehouse monitoring?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:13:59,873 - chat_service - INFO - Answer: Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Answer: Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:14:03,226 - chat_service - INFO - Turn 1: Asking question: What notification channels can you configure for monitor alerts in Databricks Lakehouse?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Turn 1: Asking question: What notification channels can you configure for monitor alerts in Databricks Lakehouse?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:14:05,370 - chat_service - INFO - Answer: In Databricks Lakehouse, you can configure monitor alerts to send notifications through various channels, including:\n\n1. Email\n2. Webhooks\n3. Applications like Slack\n4. PagerDuty\n\nThese channels allow you to receive alerts when certain metrics move out of a specified range or when there are significant changes in your data.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Answer: In Databricks Lakehouse, you can configure monitor alerts to send notifications through various channels, including:\n\n1. Email\n2. Webhooks\n3. Applications like Slack\n4. PagerDuty\n\nThese channels allow you to receive alerts when certain metrics move out of a specified range or when there are significant changes in your data.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:14:07,872 - chat_service - INFO - Turn 2: Asking question: What channels can you configure for Databricks Lakehouse monitor alerts?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Turn 2: Asking question: What channels can you configure for Databricks Lakehouse monitor alerts?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 15:14:13,866 - chat_service - INFO - Answer: I couldn't find specific documentation on the notification channels for Databricks Lakehouse monitor alerts. However, typically, you can configure alerts to send notifications via email, webhooks, and applications like Slack and PagerDuty. If you need more detailed or specific information, I recommend checking the official Databricks documentation or contacting their support.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chat_service:Answer: I couldn't find specific documentation on the notification channels for Databricks Lakehouse monitor alerts. However, typically, you can configure alerts to send notifications via email, webhooks, and applications like Slack and PagerDuty. If you need more detailed or specific information, I recommend checking the official Databricks documentation or contacting their support.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = str(output_dir / \"synthetic_evaluation_set.jsonl\")\n",
    "\n",
    "# Log the model\n",
    "# We use a function-calling agent defined in fc_agent.py that queries databricks documentation through a keyword-based retriever tool.\n",
    "# You can replace this with your own agent implementation.\n",
    "\n",
    "# Note: this configuration is specific to the code in the `fc_agent.py` file.  Your own agent will likely have a different configuration.\n",
    "agent_config = {\n",
    "    \"endpoint_name\": \"ep-gpt4o-new\",  # replace with a Model Serving endpoint that supports Chat Completions.  Can be an external model e.g., OpenAI\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "\"\"\",\n",
    "    \"max_context_chars\": 4096 * 4,\n",
    "}\n",
    "model_info = log_model(\n",
    "    agent_code_file=str(os.getcwd()+ \"/fc_agent.py\"),\n",
    "    agent_config=agent_config,\n",
    ")\n",
    "\n",
    "# Common parameters\n",
    "\n",
    "seed_questions = [\"what is lakehouse monitoring?\"] # Add more questions to this array\n",
    "\n",
    "params = {\n",
    "    \"max_turns\": 2,\n",
    "    \"model_info\": model_info,\n",
    "    \"output_file\": output_file,\n",
    "    \"agent_description\": \"A chat agent that answers questions about Databricks documentation.\",\n",
    "}\n",
    "\n",
    "# TODO: Parraellize\n",
    "for seed_question in seed_questions:\n",
    "  # Run both types of generation with different tags\n",
    "  generate_based_on_tool_outputs(**params, tag=\"tool_outputs\", seed_question=seed_question)\n",
    "  generate_based_on_response(**params, tag=\"agent_response\", seed_question=seed_question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a85dc2c0-7de5-4ac7-a1e6-7b616fc809c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a8d246-9e8c-4446-8e6c-4d8530ca9c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>request</th><th>response</th><th>metadata</th></tr></thead><tbody><tr><td>List(List(You are a helpful AI assistant engaging in a conversation., system), List(what is lakehouse monitoring?, user))</td><td>List(Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This includes setting up alerts to notify you when certain metrics or statistics move out of a specified range, indicating potential issues such as data drift or the need for model retraining.\n",
       "\n",
       "For more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., assistant)</td><td>List(null, true, what is lakehouse monitoring?, tool_outputs)</td></tr><tr><td>List(List(You are a helpful AI assistant engaging in a conversation., system), List(what is lakehouse monitoring?, user), List(Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This includes setting up alerts to notify you when certain metrics or statistics move out of a specified range, indicating potential issues such as data drift or the need for model retraining.\n",
       "\n",
       "For more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., assistant), List(How can I receive notifications for Databricks Lakehouse monitor alerts?, user))</td><td>List(To receive notifications for Databricks Lakehouse monitor alerts, you can set up alerts using Databricks SQL. Here's how you can do it:\n",
       "\n",
       "1. **Create a Databricks SQL Query**: First, create a SQL query on the monitor profile metrics table or drift metrics table. This query will define the conditions under which you want to be alerted.\n",
       "\n",
       "2. **Create a Databricks SQL Alert**: Once you have your query, create an alert for it. You can configure the alert to evaluate the query at a desired frequency.\n",
       "\n",
       "3. **Configure Notifications**: By default, email notifications are sent when an alert is triggered. However, you can also set up notifications to be sent to other applications such as Slack or PagerDuty using webhooks.\n",
       "\n",
       "4. **Quick Alert Creation from Dashboard**: You can also create an alert directly from the monitor dashboard:\n",
       "   - Find the chart for which you want to create an alert.\n",
       "   - Click the kebab menu (three vertical dots) in the upper-right corner of the chart and select \"View query.\"\n",
       "   - In the SQL editor, click the kebab menu above the editor window and select \"Create alert.\"\n",
       "   - Configure the alert and click \"Create alert.\"\n",
       "\n",
       "For more detailed steps and options, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., assistant)</td><td>List([{\"page_content\": \"# Introduction to Databricks Lakehouse Monitoring\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html\", \"score\": 2, \"chunk_id\": \"46a5182ce48b20d7617a750390b29baa\"}}, {\"page_content\": \"# Introduction to Databricks Lakehouse Monitoring\\n### Monitor alerts\\n\\nPreview  \\nThis feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).  \\nThis page describes how to create a Databricks SQL alert based on a metric from a monitor metrics table. Some common uses for monitor alerts include:  \\n* Get notified when a statistic moves out of a certain range. For example, you want to receive a notification when the fraction of missing values exceeds a certain level.\\n* Get notified of a change in the data. The drift metrics table stores statistics that track changes in the data distribution.\\n* Get notified if data has drifted in comparison to the baseline table. You can set up an alert to investigate the data changes or, for `InferenceLog` analysis, to indicate that the model should be retrained.  \\nMonitor alerts are created and used the same way as other Databricks SQL alerts. You create a [Databricks SQL query](https://docs.databricks.com/sql/user/queries/index.html) on the monitor profile metrics table or drift metrics table. You then create a Databricks SQL alert for this query. You can configure the alert to evaluate the query at a desired frequency, and send a notification if the alert is triggered. By default, email notification is sent. You can also set up a webhook or send notifications to other applications such as Slack or Pagerduty.  \\nYou can also quickly create an alert from the [monitor dashboard](https://docs.databricks.com/lakehouse-monitoring/monitor-dashboard.html) as follows:  \\n1. On the dashboard, find the chart for which you want to create an alert.\\n2. Click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) in the upper-right corner of the chart and select **View query**. The SQL editor opens.\\n3. In the SQL editor, click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) above the editor window and select **Create alert**. The **New alert** dialog opens in a new tab.\\n4. Configure the alert and click **Create alert**.  \\nNote that if the query uses parameters, then the alert is based on the default values for these parameters. You should confirm that the default values reflect the intent of the alert.  \\nFor details, see [Databricks SQL alerts](https://docs.databricks.com/sql/user/alerts/index.html).\\n\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html\", \"score\": 2, \"chunk_id\": \"ad46dd649af7864f934123978c11d492\"}}, {\"page_content\": \"# Introduction to the well-architected data lakehouse\\n### Download lakehouse reference architectures\\n#### Capabilities for your workloads\\n\\nIn addition, the Databricks lakehouse comes with management capabilities that support all workloads:  \\n* **Data and AI governance**  \\nThe central data and AI governance system in the Databricks Data Intelligence Platform is [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html). Unity Catalog provides a single place to manage data access policies that apply across all workspaces and supports all assets created or used in the lakehouse, such as tables, volumes, features ([feature store](https://docs.databricks.com/machine-learning/feature-store/index.html)), and models ([model registry](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)). Unity Catalog can also be used to [capture runtime data lineage](https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html) across queries run on Databricks.  \\nDatabricks [lakehouse monitoring](https://docs.databricks.com/lakehouse-monitoring/index.html) allows you to monitor the quality of the data in all of the tables in your account. It can also track the performance of [machine learning models and model-serving endpoints](https://docs.databricks.com/machine-learning/model-serving/monitor-diagnose-endpoints.html).  \\nFor Observability, [system tables](https://docs.databricks.com/admin/system-tables/index.html) is a Databricks-hosted analytical store of your account\\u2019s operational data. System tables can be used for historical observability across your account.\\n* **Data intelligence engine**  \\nThe Databricks Data Intelligence Platform allows your entire organization to use data and AI. It is powered by [DatabricksIQ](https://docs.databricks.com/databricksiq/index.html) and combines generative AI with the unification benefits of a lakehouse to understand the unique semantics of your data.  \\nThe [Databricks Assistant](https://docs.databricks.com/notebooks/databricks-assistant-faq.html) is available in Databricks notebooks, SQL editor, and file editor as a context-aware AI assistant for developers.  \\n* **Orchestration**  \\n[Databricks Workflows](https://docs.databricks.com/workflows/index.html) orchestrate data processing, machine learning, and analytics pipelines in the Databricks Data Intelligence Platform. Workflows has fully managed orchestration services integrated into the Databricks platform, including [Databricks Jobs](https://docs.databricks.com/workflows/index.html#what-is-databricks-jobs) to run non-interactive code in your Databricks workspace and [Delta Live Tables](https://docs.databricks.com/delta-live-tables/index.html) to build reliable and maintainable ETL pipelines.\\n\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-architecture/reference.html\", \"score\": 2, \"chunk_id\": \"95c2d07889b26b243d43b632ba0fe699\"}}, {\"page_content\": \"# Databricks release notes\\n### Databricks platform release notes\\n\\n* [May 2024](https://docs.databricks.com/release-notes/product/2024/may.html)\\n+ [Compute plane outbound IP addresses must be added to a workspace IP allow list](https://docs.databricks.com/release-notes/product/2024/may.html#compute-plane-outbound-ip-addresses-must-be-added-to-a-workspace-ip-allow-list)\\n+ [OAuth is supported in Lakehouse Federation for Snowflake](https://docs.databricks.com/release-notes/product/2024/may.html#oauth-is-supported-in-lakehouse-federation-for-snowflake)\\n+ [Bulk move and delete workspace objects from the workspace browser](https://docs.databricks.com/release-notes/product/2024/may.html#bulk-move-and-delete-workspace-objects-from-the-workspace-browser)\\n+ [New compliance and security settings APIs (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#new-compliance-and-security-settings-apis-public-preview)\\n+ [Databricks Runtime 15.2 is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-is-ga)\\n+ [New Tableau connector for Delta Sharing](https://docs.databricks.com/release-notes/product/2024/may.html#new-tableau-connector-for-delta-sharing)\\n+ [New deep learning recommendation model examples](https://docs.databricks.com/release-notes/product/2024/may.html#new-deep-learning-recommendation-model-examples)\\n+ [Bind storage credentials and external locations to specific workspaces (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#bind-storage-credentials-and-external-locations-to-specific-workspaces-public-preview)\\n+ [Git folders are GA](https://docs.databricks.com/release-notes/product/2024/may.html#git-folders-are-ga)\\n+ [Pre-trained models in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#pre-trained-models-in-unity-catalog-public-preview)\\n+ [Databricks Vector Search is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-vector-search-is-ga)\\n+ [The compliance security profile now supports AWS Graviton instance types](https://docs.databricks.com/release-notes/product/2024/may.html#the-compliance-security-profile-now-supports-aws-graviton-instance-types)\\n+ [Databricks Assistant autocomplete (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-assistant-autocomplete-public-preview)\\n+ [Meta Llama 3 support in Foundation Model Training](https://docs.databricks.com/release-notes/product/2024/may.html#meta-llama-3-support-in-foundation-model-training)\\n+ [New changes to Git folder UI](https://docs.databricks.com/release-notes/product/2024/may.html#new-changes-to-git-folder-ui)\\n+ [Compute now uses EBS GP3 volumes for autoscaling local storage](https://docs.databricks.com/release-notes/product/2024/may.html#compute-now-uses-ebs-gp3-volumes-for-autoscaling-local-storage)\\n+ [Unified Login now supported with AWS PrivateLink (Private Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#unified-login-now-supported-with-aws-privatelink-private-preview)\\n+ [Foundation Model Training (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#foundation-model-training-public-preview)\\n+ [Attribute tag values for Unity Catalog objects can now be 1000 characters long (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#attribute-tag-values-for-unity-catalog-objects-can-now-be-1000-characters-long-public-preview)\\n+ [New Previews page](https://docs.databricks.com/release-notes/product/2024/may.html#new-previews-page)\\n+ [New capabilities for Databricks Vector Search](https://docs.databricks.com/release-notes/product/2024/may.html#new-capabilities-for-databricks-vector-search)\\n+ [Credential passthrough and Hive metastore table access controls are deprecated](https://docs.databricks.com/release-notes/product/2024/may.html#credential-passthrough-and-hive-metastore-table-access-controls-are-deprecated)\\n+ [Databricks JDBC driver 2.6.38](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-jdbc-driver-2638)\\n+ [Databricks Runtime 15.2 (Beta)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-beta)\\n+ [Notebooks now detect and auto-complete column names for Spark Connect DataFrames](https://docs.databricks.com/release-notes/product/2024/may.html#notebooks-now-detect-and-auto-complete-column-names-for-spark-connect-dataframes)\\n* [April 2024](https://docs.databricks.com/release-notes/product/2024/april.html)\\n+ [Databricks Runtime 15.1 is GA](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-runtime-151-is-ga)\\n+ [Databricks Assistant: Threads & history](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-assistant-threads--history)\\n+ [Cancel pending serving endpoint updates in Model Serving](https://docs.databricks.com/release-notes/product/2024/april.html#cancel-pending-serving-endpoint-updates-in-model-serving)\\n+ [Data lineage now captures reads on tables with column masks and row-level security](https://docs.databricks.com/release-notes/product/2024/april.html#data-lineage-now-captures-reads-on-tables-with-column-masks-and-row-level-security)\\n+ [Meta Llama 3 is supported in Model Serving for AWS](https://docs.databricks.com/release-notes/product/2024/april.html#meta-llama-3-is-supported-in-model-serving-for-aws)\\n+ [Notebooks now automatically detect SQL](https://docs.databricks.com/release-notes/product/2024/april.html#notebooks-now-automatically-detect-sql)\\n+ [New columns added to the billable usage system table (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#new-columns-added-to-the-billable-usage-system-table-public-preview)\\n+ [Delta Sharing supports tables that use column mapping (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-sharing-supports-tables-that-use-column-mapping-public-preview)\\n+ [Get serving endpoint schemas (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#get-serving-endpoint-schemas-public-preview)\\n+ [Creation and installation of workspace libraries is no longer available](https://docs.databricks.com/release-notes/product/2024/april.html#creation-and-installation-of-workspace-libraries-is-no-longer-available)\\n+ [Jobs created through the UI are now queued by default](https://docs.databricks.com/release-notes/product/2024/april.html#jobs-created-through-the-ui-are-now-queued-by-default)\\n+ [Configuring access to resources from serving endpoints is GA](https://docs.databricks.com/release-notes/product/2024/april.html#configuring-access-to-resources-from-serving-endpoints-is-ga)\\n+ [Serverless compute for workflows is in public preview](https://docs.databricks.com/release-notes/product/2024/april.html#serverless-compute-for-workflows-is-in-public-preview)\\n+ [Lakehouse Federation supports foreign tables with case-sensitive identifiers](https://docs.databricks.com/release-notes/product/2024/april.html#lakehouse-federation-supports-foreign-tables-with-case-sensitive-identifiers)\\n+ [Compute cloning now clones any libraries installed on the original compute](https://docs.databricks.com/release-notes/product/2024/april.html#compute-cloning-now-clones-any-libraries-installed-on-the-original-compute)\\n+ [Route optimization is available for serving endpoints](https://docs.databricks.com/release-notes/product/2024/april.html#route-optimization-is-available-for-serving-endpoints)\\n+ [Delta Live Tables notebook developer experience improvements (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-live-tables-notebook-developer-experience-improvements-public-preview)\\n+ [Databricks on AWS GovCloud (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-on-aws-govcloud-public-preview)\\n* [March 2024](https://docs.databricks.com/release-notes/product/2024/march.html)\\n+ [DBRX Base and DBRX Instruct are now available in Model Serving](https://docs.databricks.com/release-notes/product/2024/march.html#dbrx-base-and-dbrx-instruct-are-now-available-in-model-serving)\\n+ [Model Serving is HIPAA compliant in all regions](https://docs.databricks.com/release-notes/product/2024/march.html#model-serving-is-hipaa-compliant-in-all-regions)\\n+ [Provisioned throughput in Foundation Model APIs is GA and HIPAA compliant](https://docs.databricks.com/release-notes/product/2024/march.html#provisioned-throughput-in-foundation-model-apis-is-ga-and-hipaa-compliant)\\n+ [MLflow now enforces quota limits for experiments and runs](https://docs.databricks.com/release-notes/product/2024/march.html#mlflow-now-enforces-quota-limits-for-experiments-and-runs)\\n+ [The Jobs UI is updated to better manage jobs deployed by Databricks Asset Bundles](https://docs.databricks.com/release-notes/product/2024/march.html#the-jobs-ui-is-updated-to-better-manage-jobs-deployed-by-databricks-asset-bundles)\\n+ [Google Cloud Vertex AI supported as model provider for external models](https://docs.databricks.com/release-notes/product/2024/march.html#google-cloud-vertex-ai-supported-as-model-provider-for-external-models)\\n+ [Access resources from serving endpoints using instance profiles is GA](https://docs.databricks.com/release-notes/product/2024/march.html#access-resources-from-serving-endpoints-using-instance-profiles-is-ga)\\n+ [Interactive notebook debugging](https://docs.databricks.com/release-notes/product/2024/march.html#interactive-notebook-debugging)\\n+ [Self-service sign-up for private exchange providers in Marketplace](https://docs.databricks.com/release-notes/product/2024/march.html#self-service-sign-up-for-private-exchange-providers-in-marketplace)\\n+ [Databricks Runtime 15.0 is GA](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-is-ga)\\n+ [Databricks Repos changed to Git folders](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-repos-changed-to-git-folders)\\n+ [Databricks Runtime 14.1 and 14.2 series support extended](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-141-and-142-series-support-extended)\\n+ [Databricks ODBC driver 2.8.0](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-odbc-driver-280)\\n+ [SQL warehouses for notebooks is GA](https://docs.databricks.com/release-notes/product/2024/march.html#sql-warehouses-for-notebooks-is-ga)\\n+ [Delegate the ability to view an object\\u2019s metadata in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/march.html#delegate-the-ability-to-view-an-objects-metadata-in-unity-catalog-public-preview)\\n+ [Databricks Runtime 15.0 (Beta)](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-beta)\\n+ [Databricks Runtime 14.0 series support ends](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-140-series-support-ends)\\n+ [New computation for sys.path and CWD in Repos](https://docs.databricks.com/release-notes/product/2024/march.html#new-computation-for-syspath-and-cwd-in-repos)\\n+ [Fe\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/release-notes/product/index.html\", \"score\": 2, \"chunk_id\": \"3c7d1aad122c45b35c16a852e6cc35b5\"}}], false, null, tool_outputs)</td></tr><tr><td>List(List(You are a helpful AI assistant engaging in a conversation., system), List(what is lakehouse monitoring?, user), List(Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This includes setting up alerts to notify you when certain metrics or statistics move out of a specified range, indicating potential issues such as data drift or the need for model retraining.\n",
       "\n",
       "For more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., assistant), List(How can I receive notifications for Databricks Lakehouse monitor alerts?, user), List(To receive notifications for Databricks Lakehouse monitor alerts, you can set up alerts using Databricks SQL. Here's how you can do it:\n",
       "\n",
       "1. **Create a Databricks SQL Query**: First, create a SQL query on the monitor profile metrics table or drift metrics table. This query will define the conditions under which you want to be alerted.\n",
       "\n",
       "2. **Create a Databricks SQL Alert**: Once you have your query, create an alert for it. You can configure the alert to evaluate the query at a desired frequency.\n",
       "\n",
       "3. **Configure Notifications**: By default, email notifications are sent when an alert is triggered. However, you can also set up notifications to be sent to other applications such as Slack or PagerDuty using webhooks.\n",
       "\n",
       "4. **Quick Alert Creation from Dashboard**: You can also create an alert directly from the monitor dashboard:\n",
       "   - Find the chart for which you want to create an alert.\n",
       "   - Click the kebab menu (three vertical dots) in the upper-right corner of the chart and select \"View query.\"\n",
       "   - In the SQL editor, click the kebab menu above the editor window and select \"Create alert.\"\n",
       "   - Configure the alert and click \"Create alert.\"\n",
       "\n",
       "For more detailed steps and options, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., assistant), List(How do I create an alert from the monitor dashboard?, user))</td><td>List(To create an alert from the monitor dashboard in Databricks, follow these steps:\n",
       "\n",
       "1. **Access the Monitor Dashboard**: Navigate to the monitor dashboard where you have the visualizations of your data metrics.\n",
       "\n",
       "2. **Select the Chart**: Identify the chart for which you want to create an alert. This chart should represent the data or metric you want to monitor.\n",
       "\n",
       "3. **Open the Chart Menu**: Click on the kebab menu (three vertical dots) located in the upper-right corner of the chart.\n",
       "\n",
       "4. **View the Query**: From the dropdown menu, select \"View query.\" This action will open the SQL editor with the query used to generate the chart.\n",
       "\n",
       "5. **Create the Alert**: In the SQL editor, locate the kebab menu above the editor window. Click on it and select \"Create alert.\"\n",
       "\n",
       "6. **Configure the Alert**: You will be prompted to configure the alert settings. This includes setting the conditions under which the alert should trigger, the frequency of evaluation, and the notification settings.\n",
       "\n",
       "7. **Finalize the Alert**: After configuring the alert, click \"Create alert\" to finalize and activate it.\n",
       "\n",
       "By following these steps, you can easily set up alerts directly from the monitor dashboard, ensuring you receive timely notifications based on your data metrics., assistant)</td><td>List([{\"page_content\": \"# Introduction to Databricks Lakehouse Monitoring\\n### Monitor alerts\\n\\nPreview  \\nThis feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).  \\nThis page describes how to create a Databricks SQL alert based on a metric from a monitor metrics table. Some common uses for monitor alerts include:  \\n* Get notified when a statistic moves out of a certain range. For example, you want to receive a notification when the fraction of missing values exceeds a certain level.\\n* Get notified of a change in the data. The drift metrics table stores statistics that track changes in the data distribution.\\n* Get notified if data has drifted in comparison to the baseline table. You can set up an alert to investigate the data changes or, for `InferenceLog` analysis, to indicate that the model should be retrained.  \\nMonitor alerts are created and used the same way as other Databricks SQL alerts. You create a [Databricks SQL query](https://docs.databricks.com/sql/user/queries/index.html) on the monitor profile metrics table or drift metrics table. You then create a Databricks SQL alert for this query. You can configure the alert to evaluate the query at a desired frequency, and send a notification if the alert is triggered. By default, email notification is sent. You can also set up a webhook or send notifications to other applications such as Slack or Pagerduty.  \\nYou can also quickly create an alert from the [monitor dashboard](https://docs.databricks.com/lakehouse-monitoring/monitor-dashboard.html) as follows:  \\n1. On the dashboard, find the chart for which you want to create an alert.\\n2. Click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) in the upper-right corner of the chart and select **View query**. The SQL editor opens.\\n3. In the SQL editor, click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) above the editor window and select **Create alert**. The **New alert** dialog opens in a new tab.\\n4. Configure the alert and click **Create alert**.  \\nNote that if the query uses parameters, then the alert is based on the default values for these parameters. You should confirm that the default values reflect the intent of the alert.  \\nFor details, see [Databricks SQL alerts](https://docs.databricks.com/sql/user/alerts/index.html).\\n\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html\", \"score\": 5, \"chunk_id\": \"ad46dd649af7864f934123978c11d492\"}}, {\"page_content\": \"# Databricks release notes\\n### Databricks platform release notes\\n\\n* [May 2024](https://docs.databricks.com/release-notes/product/2024/may.html)\\n+ [Compute plane outbound IP addresses must be added to a workspace IP allow list](https://docs.databricks.com/release-notes/product/2024/may.html#compute-plane-outbound-ip-addresses-must-be-added-to-a-workspace-ip-allow-list)\\n+ [OAuth is supported in Lakehouse Federation for Snowflake](https://docs.databricks.com/release-notes/product/2024/may.html#oauth-is-supported-in-lakehouse-federation-for-snowflake)\\n+ [Bulk move and delete workspace objects from the workspace browser](https://docs.databricks.com/release-notes/product/2024/may.html#bulk-move-and-delete-workspace-objects-from-the-workspace-browser)\\n+ [New compliance and security settings APIs (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#new-compliance-and-security-settings-apis-public-preview)\\n+ [Databricks Runtime 15.2 is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-is-ga)\\n+ [New Tableau connector for Delta Sharing](https://docs.databricks.com/release-notes/product/2024/may.html#new-tableau-connector-for-delta-sharing)\\n+ [New deep learning recommendation model examples](https://docs.databricks.com/release-notes/product/2024/may.html#new-deep-learning-recommendation-model-examples)\\n+ [Bind storage credentials and external locations to specific workspaces (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#bind-storage-credentials-and-external-locations-to-specific-workspaces-public-preview)\\n+ [Git folders are GA](https://docs.databricks.com/release-notes/product/2024/may.html#git-folders-are-ga)\\n+ [Pre-trained models in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#pre-trained-models-in-unity-catalog-public-preview)\\n+ [Databricks Vector Search is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-vector-search-is-ga)\\n+ [The compliance security profile now supports AWS Graviton instance types](https://docs.databricks.com/release-notes/product/2024/may.html#the-compliance-security-profile-now-supports-aws-graviton-instance-types)\\n+ [Databricks Assistant autocomplete (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-assistant-autocomplete-public-preview)\\n+ [Meta Llama 3 support in Foundation Model Training](https://docs.databricks.com/release-notes/product/2024/may.html#meta-llama-3-support-in-foundation-model-training)\\n+ [New changes to Git folder UI](https://docs.databricks.com/release-notes/product/2024/may.html#new-changes-to-git-folder-ui)\\n+ [Compute now uses EBS GP3 volumes for autoscaling local storage](https://docs.databricks.com/release-notes/product/2024/may.html#compute-now-uses-ebs-gp3-volumes-for-autoscaling-local-storage)\\n+ [Unified Login now supported with AWS PrivateLink (Private Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#unified-login-now-supported-with-aws-privatelink-private-preview)\\n+ [Foundation Model Training (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#foundation-model-training-public-preview)\\n+ [Attribute tag values for Unity Catalog objects can now be 1000 characters long (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#attribute-tag-values-for-unity-catalog-objects-can-now-be-1000-characters-long-public-preview)\\n+ [New Previews page](https://docs.databricks.com/release-notes/product/2024/may.html#new-previews-page)\\n+ [New capabilities for Databricks Vector Search](https://docs.databricks.com/release-notes/product/2024/may.html#new-capabilities-for-databricks-vector-search)\\n+ [Credential passthrough and Hive metastore table access controls are deprecated](https://docs.databricks.com/release-notes/product/2024/may.html#credential-passthrough-and-hive-metastore-table-access-controls-are-deprecated)\\n+ [Databricks JDBC driver 2.6.38](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-jdbc-driver-2638)\\n+ [Databricks Runtime 15.2 (Beta)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-beta)\\n+ [Notebooks now detect and auto-complete column names for Spark Connect DataFrames](https://docs.databricks.com/release-notes/product/2024/may.html#notebooks-now-detect-and-auto-complete-column-names-for-spark-connect-dataframes)\\n* [April 2024](https://docs.databricks.com/release-notes/product/2024/april.html)\\n+ [Databricks Runtime 15.1 is GA](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-runtime-151-is-ga)\\n+ [Databricks Assistant: Threads & history](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-assistant-threads--history)\\n+ [Cancel pending serving endpoint updates in Model Serving](https://docs.databricks.com/release-notes/product/2024/april.html#cancel-pending-serving-endpoint-updates-in-model-serving)\\n+ [Data lineage now captures reads on tables with column masks and row-level security](https://docs.databricks.com/release-notes/product/2024/april.html#data-lineage-now-captures-reads-on-tables-with-column-masks-and-row-level-security)\\n+ [Meta Llama 3 is supported in Model Serving for AWS](https://docs.databricks.com/release-notes/product/2024/april.html#meta-llama-3-is-supported-in-model-serving-for-aws)\\n+ [Notebooks now automatically detect SQL](https://docs.databricks.com/release-notes/product/2024/april.html#notebooks-now-automatically-detect-sql)\\n+ [New columns added to the billable usage system table (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#new-columns-added-to-the-billable-usage-system-table-public-preview)\\n+ [Delta Sharing supports tables that use column mapping (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-sharing-supports-tables-that-use-column-mapping-public-preview)\\n+ [Get serving endpoint schemas (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#get-serving-endpoint-schemas-public-preview)\\n+ [Creation and installation of workspace libraries is no longer available](https://docs.databricks.com/release-notes/product/2024/april.html#creation-and-installation-of-workspace-libraries-is-no-longer-available)\\n+ [Jobs created through the UI are now queued by default](https://docs.databricks.com/release-notes/product/2024/april.html#jobs-created-through-the-ui-are-now-queued-by-default)\\n+ [Configuring access to resources from serving endpoints is GA](https://docs.databricks.com/release-notes/product/2024/april.html#configuring-access-to-resources-from-serving-endpoints-is-ga)\\n+ [Serverless compute for workflows is in public preview](https://docs.databricks.com/release-notes/product/2024/april.html#serverless-compute-for-workflows-is-in-public-preview)\\n+ [Lakehouse Federation supports foreign tables with case-sensitive identifiers](https://docs.databricks.com/release-notes/product/2024/april.html#lakehouse-federation-supports-foreign-tables-with-case-sensitive-identifiers)\\n+ [Compute cloning now clones any libraries installed on the original compute](https://docs.databricks.com/release-notes/product/2024/april.html#compute-cloning-now-clones-any-libraries-installed-on-the-original-compute)\\n+ [Route optimization is available for serving endpoints](https://docs.databricks.com/release-notes/product/2024/april.html#route-optimization-is-available-for-serving-endpoints)\\n+ [Delta Live Tables notebook developer experience improvements (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-live-tables-notebook-developer-experience-improvements-public-preview)\\n+ [Databricks on AWS GovCloud (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-on-aws-govcloud-public-preview)\\n* [March 2024](https://docs.databricks.com/release-notes/product/2024/march.html)\\n+ [DBRX Base and DBRX Instruct are now available in Model Serving](https://docs.databricks.com/release-notes/product/2024/march.html#dbrx-base-and-dbrx-instruct-are-now-available-in-model-serving)\\n+ [Model Serving is HIPAA compliant in all regions](https://docs.databricks.com/release-notes/product/2024/march.html#model-serving-is-hipaa-compliant-in-all-regions)\\n+ [Provisioned throughput in Foundation Model APIs is GA and HIPAA compliant](https://docs.databricks.com/release-notes/product/2024/march.html#provisioned-throughput-in-foundation-model-apis-is-ga-and-hipaa-compliant)\\n+ [MLflow now enforces quota limits for experiments and runs](https://docs.databricks.com/release-notes/product/2024/march.html#mlflow-now-enforces-quota-limits-for-experiments-and-runs)\\n+ [The Jobs UI is updated to better manage jobs deployed by Databricks Asset Bundles](https://docs.databricks.com/release-notes/product/2024/march.html#the-jobs-ui-is-updated-to-better-manage-jobs-deployed-by-databricks-asset-bundles)\\n+ [Google Cloud Vertex AI supported as model provider for external models](https://docs.databricks.com/release-notes/product/2024/march.html#google-cloud-vertex-ai-supported-as-model-provider-for-external-models)\\n+ [Access resources from serving endpoints using instance profiles is GA](https://docs.databricks.com/release-notes/product/2024/march.html#access-resources-from-serving-endpoints-using-instance-profiles-is-ga)\\n+ [Interactive notebook debugging](https://docs.databricks.com/release-notes/product/2024/march.html#interactive-notebook-debugging)\\n+ [Self-service sign-up for private exchange providers in Marketplace](https://docs.databricks.com/release-notes/product/2024/march.html#self-service-sign-up-for-private-exchange-providers-in-marketplace)\\n+ [Databricks Runtime 15.0 is GA](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-is-ga)\\n+ [Databricks Repos changed to Git folders](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-repos-changed-to-git-folders)\\n+ [Databricks Runtime 14.1 and 14.2 series support extended](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-141-and-142-series-support-extended)\\n+ [Databricks ODBC driver 2.8.0](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-odbc-driver-280)\\n+ [SQL warehouses for notebooks is GA](https://docs.databricks.com/release-notes/product/2024/march.html#sql-warehouses-for-notebooks-is-ga)\\n+ [Delegate the ability to view an object\\u2019s metadata in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/march.html#delegate-the-ability-to-view-an-objects-metadata-in-unity-catalog-public-preview)\\n+ [Databricks Runtime 15.0 (Beta)](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-beta)\\n+ [Databricks Runtime 14.0 series support ends](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-140-series-support-ends)\\n+ [New computation for sys.path and CWD in Repos](https://docs.databricks.com/release-notes/product/2024/march.html#new-computation-for-syspath-and-cwd-in-repos)\\n+ [Feature Serving is GA](https://docs.databricks.com/release-notes/product/2024/march.html#feature-serving-is-ga)\\n+ [Predictive optimization available in more regions](https://docs.databricks.com/release-notes/product/2024/march.html#predictive-optimization-available-in-more-regions)\\n* [February 2024](https://docs.databricks.com/release-notes/product/2024/february.html)\\n+ [Use Delta Live Tables in Feature Engineering (Public Preview)](https://docs.databricks.com/release-notes/product/2024/february.html#use-delta-live-tables-in-feature-engineering-public-preview)\\n+ [Restrict creating a personal access token for a service principal](https://docs.databricks.com/release-notes/product/2024/february.html#restrict-creating-a-personal-access-token-for-a-service-principal)\\n+ [Restrict changing a job owner and the run as setting](https://docs.databricks.com/release-notes/product/2024/february.html#restrict-changing-a-job-owner-and-the-run-as-setting)\\n+ [Automatic cluster update is enabled if the compliance security profile is enabled (GA)](https://docs.databricks.com/release-notes/product/2024/february.html#automatic-cluster-update-is-enabled-if-the-compliance-security-profile-is-enabled-ga-for-aws-cluster-update-is-changed-not-new-but-is-ga-now)\\n+ [Account admins can enable enhanced security and compliance features (Public Preview)](https://docs.databricks.com/release-notes/product/2024/february.html#account-admins-can-enable-enhanced-security-and-compliance-features-public-preview)\\n+ [Support for Cloudflare R2 storage to avoid cross-region egress fees (Public Preview)](https://docs.databricks.com/release-notes/product/2024/february.html#support-for-cloudflare-r2-storage-to-avoid-cross-region-egress-fees-public-preview)\\n+ [Notebooks for monitoring and managing Delta Sharing egress costs are now available](https://docs.databricks.com/release-notes/product/2024/february.html#notebooks-for-monitoring-and-managing-delta-sharing-egress-costs-are-now-available)\\n+ [Add data UI supports XML file format](https://docs.databricks.com/release-notes/product/2024/february.html#add-data-ui-supports-xml-file-format)\\n+ [Support for cloud storage firewall from serverless compute (Public Preview)](https://docs.databricks.com/release-notes/product/2024/february.html#support-for-cloud-storage-firewall-from-serverless-compute-public-preview)\\n+ [Use AI Functions to invoke a generative AI model from Foundation Model APIs](https://docs.databricks.com/release-notes/product/2024/february.html#use-ai-functions-to-invoke-a-generative-ai-model-from-foundation-model-apis)\\n+ [Unity Catalog volumes are GA](https://docs.databricks.com/release-notes/product/2024/february.html#unity-catalog-volumes-are-ga)\\n+ [Full-page AI-powered search](https://docs.databricks.com/release-notes/product/2024/february.html#full-page-ai-powered\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/release-notes/product/index.html\", \"score\": 5, \"chunk_id\": \"3c7d1aad122c45b35c16a852e6cc35b5\"}}], false, null, tool_outputs)</td></tr><tr><td>List(List(You are a helpful AI assistant engaging in a conversation., system), List(what is lakehouse monitoring?, user))</td><td>List(Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n",
       "\n",
       "For more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., assistant)</td><td>List(null, true, what is lakehouse monitoring?, agent_response)</td></tr><tr><td>List(List(You are a helpful AI assistant engaging in a conversation., system), List(what is lakehouse monitoring?, user), List(Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n",
       "\n",
       "For more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., assistant), List(What notification channels can you configure for monitor alerts in Databricks Lakehouse?, user))</td><td>List(In Databricks Lakehouse, you can configure monitor alerts to send notifications through various channels, including:\n",
       "\n",
       "1. Email\n",
       "2. Webhooks\n",
       "3. Applications like Slack\n",
       "4. PagerDuty\n",
       "\n",
       "These channels allow you to receive alerts when certain metrics move out of a specified range or when there are significant changes in your data., assistant)</td><td>List(Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n",
       "\n",
       "For more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., false, null, agent_response)</td></tr><tr><td>List(List(You are a helpful AI assistant engaging in a conversation., system), List(what is lakehouse monitoring?, user), List(Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n",
       "\n",
       "For more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html)., assistant), List(What notification channels can you configure for monitor alerts in Databricks Lakehouse?, user), List(In Databricks Lakehouse, you can configure monitor alerts to send notifications through various channels, including:\n",
       "\n",
       "1. Email\n",
       "2. Webhooks\n",
       "3. Applications like Slack\n",
       "4. PagerDuty\n",
       "\n",
       "These channels allow you to receive alerts when certain metrics move out of a specified range or when there are significant changes in your data., assistant), List(What channels can you configure for Databricks Lakehouse monitor alerts?, user))</td><td>List(I couldn't find specific documentation on the notification channels for Databricks Lakehouse monitor alerts. However, typically, you can configure alerts to send notifications via email, webhooks, and applications like Slack and PagerDuty. If you need more detailed or specific information, I recommend checking the official Databricks documentation or contacting their support., assistant)</td><td>List(In Databricks Lakehouse, you can configure monitor alerts to send notifications through various channels, including:\n",
       "\n",
       "1. Email\n",
       "2. Webhooks\n",
       "3. Applications like Slack\n",
       "4. PagerDuty\n",
       "\n",
       "These channels allow you to receive alerts when certain metrics move out of a specified range or when there are significant changes in your data., false, null, agent_response)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          [
           "You are a helpful AI assistant engaging in a conversation.",
           "system"
          ],
          [
           "what is lakehouse monitoring?",
           "user"
          ]
         ],
         [
          "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This includes setting up alerts to notify you when certain metrics or statistics move out of a specified range, indicating potential issues such as data drift or the need for model retraining.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
          "assistant"
         ],
         [
          null,
          true,
          "what is lakehouse monitoring?",
          "tool_outputs"
         ]
        ],
        [
         [
          [
           "You are a helpful AI assistant engaging in a conversation.",
           "system"
          ],
          [
           "what is lakehouse monitoring?",
           "user"
          ],
          [
           "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This includes setting up alerts to notify you when certain metrics or statistics move out of a specified range, indicating potential issues such as data drift or the need for model retraining.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
           "assistant"
          ],
          [
           "How can I receive notifications for Databricks Lakehouse monitor alerts?",
           "user"
          ]
         ],
         [
          "To receive notifications for Databricks Lakehouse monitor alerts, you can set up alerts using Databricks SQL. Here's how you can do it:\n\n1. **Create a Databricks SQL Query**: First, create a SQL query on the monitor profile metrics table or drift metrics table. This query will define the conditions under which you want to be alerted.\n\n2. **Create a Databricks SQL Alert**: Once you have your query, create an alert for it. You can configure the alert to evaluate the query at a desired frequency.\n\n3. **Configure Notifications**: By default, email notifications are sent when an alert is triggered. However, you can also set up notifications to be sent to other applications such as Slack or PagerDuty using webhooks.\n\n4. **Quick Alert Creation from Dashboard**: You can also create an alert directly from the monitor dashboard:\n   - Find the chart for which you want to create an alert.\n   - Click the kebab menu (three vertical dots) in the upper-right corner of the chart and select \"View query.\"\n   - In the SQL editor, click the kebab menu above the editor window and select \"Create alert.\"\n   - Configure the alert and click \"Create alert.\"\n\nFor more detailed steps and options, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
          "assistant"
         ],
         [
          "[{\"page_content\": \"# Introduction to Databricks Lakehouse Monitoring\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html\", \"score\": 2, \"chunk_id\": \"46a5182ce48b20d7617a750390b29baa\"}}, {\"page_content\": \"# Introduction to Databricks Lakehouse Monitoring\\n### Monitor alerts\\n\\nPreview  \\nThis feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).  \\nThis page describes how to create a Databricks SQL alert based on a metric from a monitor metrics table. Some common uses for monitor alerts include:  \\n* Get notified when a statistic moves out of a certain range. For example, you want to receive a notification when the fraction of missing values exceeds a certain level.\\n* Get notified of a change in the data. The drift metrics table stores statistics that track changes in the data distribution.\\n* Get notified if data has drifted in comparison to the baseline table. You can set up an alert to investigate the data changes or, for `InferenceLog` analysis, to indicate that the model should be retrained.  \\nMonitor alerts are created and used the same way as other Databricks SQL alerts. You create a [Databricks SQL query](https://docs.databricks.com/sql/user/queries/index.html) on the monitor profile metrics table or drift metrics table. You then create a Databricks SQL alert for this query. You can configure the alert to evaluate the query at a desired frequency, and send a notification if the alert is triggered. By default, email notification is sent. You can also set up a webhook or send notifications to other applications such as Slack or Pagerduty.  \\nYou can also quickly create an alert from the [monitor dashboard](https://docs.databricks.com/lakehouse-monitoring/monitor-dashboard.html) as follows:  \\n1. On the dashboard, find the chart for which you want to create an alert.\\n2. Click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) in the upper-right corner of the chart and select **View query**. The SQL editor opens.\\n3. In the SQL editor, click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) above the editor window and select **Create alert**. The **New alert** dialog opens in a new tab.\\n4. Configure the alert and click **Create alert**.  \\nNote that if the query uses parameters, then the alert is based on the default values for these parameters. You should confirm that the default values reflect the intent of the alert.  \\nFor details, see [Databricks SQL alerts](https://docs.databricks.com/sql/user/alerts/index.html).\\n\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html\", \"score\": 2, \"chunk_id\": \"ad46dd649af7864f934123978c11d492\"}}, {\"page_content\": \"# Introduction to the well-architected data lakehouse\\n### Download lakehouse reference architectures\\n#### Capabilities for your workloads\\n\\nIn addition, the Databricks lakehouse comes with management capabilities that support all workloads:  \\n* **Data and AI governance**  \\nThe central data and AI governance system in the Databricks Data Intelligence Platform is [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html). Unity Catalog provides a single place to manage data access policies that apply across all workspaces and supports all assets created or used in the lakehouse, such as tables, volumes, features ([feature store](https://docs.databricks.com/machine-learning/feature-store/index.html)), and models ([model registry](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)). Unity Catalog can also be used to [capture runtime data lineage](https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html) across queries run on Databricks.  \\nDatabricks [lakehouse monitoring](https://docs.databricks.com/lakehouse-monitoring/index.html) allows you to monitor the quality of the data in all of the tables in your account. It can also track the performance of [machine learning models and model-serving endpoints](https://docs.databricks.com/machine-learning/model-serving/monitor-diagnose-endpoints.html).  \\nFor Observability, [system tables](https://docs.databricks.com/admin/system-tables/index.html) is a Databricks-hosted analytical store of your account\\u2019s operational data. System tables can be used for historical observability across your account.\\n* **Data intelligence engine**  \\nThe Databricks Data Intelligence Platform allows your entire organization to use data and AI. It is powered by [DatabricksIQ](https://docs.databricks.com/databricksiq/index.html) and combines generative AI with the unification benefits of a lakehouse to understand the unique semantics of your data.  \\nThe [Databricks Assistant](https://docs.databricks.com/notebooks/databricks-assistant-faq.html) is available in Databricks notebooks, SQL editor, and file editor as a context-aware AI assistant for developers.  \\n* **Orchestration**  \\n[Databricks Workflows](https://docs.databricks.com/workflows/index.html) orchestrate data processing, machine learning, and analytics pipelines in the Databricks Data Intelligence Platform. Workflows has fully managed orchestration services integrated into the Databricks platform, including [Databricks Jobs](https://docs.databricks.com/workflows/index.html#what-is-databricks-jobs) to run non-interactive code in your Databricks workspace and [Delta Live Tables](https://docs.databricks.com/delta-live-tables/index.html) to build reliable and maintainable ETL pipelines.\\n\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-architecture/reference.html\", \"score\": 2, \"chunk_id\": \"95c2d07889b26b243d43b632ba0fe699\"}}, {\"page_content\": \"# Databricks release notes\\n### Databricks platform release notes\\n\\n* [May 2024](https://docs.databricks.com/release-notes/product/2024/may.html)\\n+ [Compute plane outbound IP addresses must be added to a workspace IP allow list](https://docs.databricks.com/release-notes/product/2024/may.html#compute-plane-outbound-ip-addresses-must-be-added-to-a-workspace-ip-allow-list)\\n+ [OAuth is supported in Lakehouse Federation for Snowflake](https://docs.databricks.com/release-notes/product/2024/may.html#oauth-is-supported-in-lakehouse-federation-for-snowflake)\\n+ [Bulk move and delete workspace objects from the workspace browser](https://docs.databricks.com/release-notes/product/2024/may.html#bulk-move-and-delete-workspace-objects-from-the-workspace-browser)\\n+ [New compliance and security settings APIs (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#new-compliance-and-security-settings-apis-public-preview)\\n+ [Databricks Runtime 15.2 is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-is-ga)\\n+ [New Tableau connector for Delta Sharing](https://docs.databricks.com/release-notes/product/2024/may.html#new-tableau-connector-for-delta-sharing)\\n+ [New deep learning recommendation model examples](https://docs.databricks.com/release-notes/product/2024/may.html#new-deep-learning-recommendation-model-examples)\\n+ [Bind storage credentials and external locations to specific workspaces (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#bind-storage-credentials-and-external-locations-to-specific-workspaces-public-preview)\\n+ [Git folders are GA](https://docs.databricks.com/release-notes/product/2024/may.html#git-folders-are-ga)\\n+ [Pre-trained models in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#pre-trained-models-in-unity-catalog-public-preview)\\n+ [Databricks Vector Search is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-vector-search-is-ga)\\n+ [The compliance security profile now supports AWS Graviton instance types](https://docs.databricks.com/release-notes/product/2024/may.html#the-compliance-security-profile-now-supports-aws-graviton-instance-types)\\n+ [Databricks Assistant autocomplete (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-assistant-autocomplete-public-preview)\\n+ [Meta Llama 3 support in Foundation Model Training](https://docs.databricks.com/release-notes/product/2024/may.html#meta-llama-3-support-in-foundation-model-training)\\n+ [New changes to Git folder UI](https://docs.databricks.com/release-notes/product/2024/may.html#new-changes-to-git-folder-ui)\\n+ [Compute now uses EBS GP3 volumes for autoscaling local storage](https://docs.databricks.com/release-notes/product/2024/may.html#compute-now-uses-ebs-gp3-volumes-for-autoscaling-local-storage)\\n+ [Unified Login now supported with AWS PrivateLink (Private Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#unified-login-now-supported-with-aws-privatelink-private-preview)\\n+ [Foundation Model Training (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#foundation-model-training-public-preview)\\n+ [Attribute tag values for Unity Catalog objects can now be 1000 characters long (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#attribute-tag-values-for-unity-catalog-objects-can-now-be-1000-characters-long-public-preview)\\n+ [New Previews page](https://docs.databricks.com/release-notes/product/2024/may.html#new-previews-page)\\n+ [New capabilities for Databricks Vector Search](https://docs.databricks.com/release-notes/product/2024/may.html#new-capabilities-for-databricks-vector-search)\\n+ [Credential passthrough and Hive metastore table access controls are deprecated](https://docs.databricks.com/release-notes/product/2024/may.html#credential-passthrough-and-hive-metastore-table-access-controls-are-deprecated)\\n+ [Databricks JDBC driver 2.6.38](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-jdbc-driver-2638)\\n+ [Databricks Runtime 15.2 (Beta)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-beta)\\n+ [Notebooks now detect and auto-complete column names for Spark Connect DataFrames](https://docs.databricks.com/release-notes/product/2024/may.html#notebooks-now-detect-and-auto-complete-column-names-for-spark-connect-dataframes)\\n* [April 2024](https://docs.databricks.com/release-notes/product/2024/april.html)\\n+ [Databricks Runtime 15.1 is GA](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-runtime-151-is-ga)\\n+ [Databricks Assistant: Threads & history](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-assistant-threads--history)\\n+ [Cancel pending serving endpoint updates in Model Serving](https://docs.databricks.com/release-notes/product/2024/april.html#cancel-pending-serving-endpoint-updates-in-model-serving)\\n+ [Data lineage now captures reads on tables with column masks and row-level security](https://docs.databricks.com/release-notes/product/2024/april.html#data-lineage-now-captures-reads-on-tables-with-column-masks-and-row-level-security)\\n+ [Meta Llama 3 is supported in Model Serving for AWS](https://docs.databricks.com/release-notes/product/2024/april.html#meta-llama-3-is-supported-in-model-serving-for-aws)\\n+ [Notebooks now automatically detect SQL](https://docs.databricks.com/release-notes/product/2024/april.html#notebooks-now-automatically-detect-sql)\\n+ [New columns added to the billable usage system table (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#new-columns-added-to-the-billable-usage-system-table-public-preview)\\n+ [Delta Sharing supports tables that use column mapping (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-sharing-supports-tables-that-use-column-mapping-public-preview)\\n+ [Get serving endpoint schemas (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#get-serving-endpoint-schemas-public-preview)\\n+ [Creation and installation of workspace libraries is no longer available](https://docs.databricks.com/release-notes/product/2024/april.html#creation-and-installation-of-workspace-libraries-is-no-longer-available)\\n+ [Jobs created through the UI are now queued by default](https://docs.databricks.com/release-notes/product/2024/april.html#jobs-created-through-the-ui-are-now-queued-by-default)\\n+ [Configuring access to resources from serving endpoints is GA](https://docs.databricks.com/release-notes/product/2024/april.html#configuring-access-to-resources-from-serving-endpoints-is-ga)\\n+ [Serverless compute for workflows is in public preview](https://docs.databricks.com/release-notes/product/2024/april.html#serverless-compute-for-workflows-is-in-public-preview)\\n+ [Lakehouse Federation supports foreign tables with case-sensitive identifiers](https://docs.databricks.com/release-notes/product/2024/april.html#lakehouse-federation-supports-foreign-tables-with-case-sensitive-identifiers)\\n+ [Compute cloning now clones any libraries installed on the original compute](https://docs.databricks.com/release-notes/product/2024/april.html#compute-cloning-now-clones-any-libraries-installed-on-the-original-compute)\\n+ [Route optimization is available for serving endpoints](https://docs.databricks.com/release-notes/product/2024/april.html#route-optimization-is-available-for-serving-endpoints)\\n+ [Delta Live Tables notebook developer experience improvements (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-live-tables-notebook-developer-experience-improvements-public-preview)\\n+ [Databricks on AWS GovCloud (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-on-aws-govcloud-public-preview)\\n* [March 2024](https://docs.databricks.com/release-notes/product/2024/march.html)\\n+ [DBRX Base and DBRX Instruct are now available in Model Serving](https://docs.databricks.com/release-notes/product/2024/march.html#dbrx-base-and-dbrx-instruct-are-now-available-in-model-serving)\\n+ [Model Serving is HIPAA compliant in all regions](https://docs.databricks.com/release-notes/product/2024/march.html#model-serving-is-hipaa-compliant-in-all-regions)\\n+ [Provisioned throughput in Foundation Model APIs is GA and HIPAA compliant](https://docs.databricks.com/release-notes/product/2024/march.html#provisioned-throughput-in-foundation-model-apis-is-ga-and-hipaa-compliant)\\n+ [MLflow now enforces quota limits for experiments and runs](https://docs.databricks.com/release-notes/product/2024/march.html#mlflow-now-enforces-quota-limits-for-experiments-and-runs)\\n+ [The Jobs UI is updated to better manage jobs deployed by Databricks Asset Bundles](https://docs.databricks.com/release-notes/product/2024/march.html#the-jobs-ui-is-updated-to-better-manage-jobs-deployed-by-databricks-asset-bundles)\\n+ [Google Cloud Vertex AI supported as model provider for external models](https://docs.databricks.com/release-notes/product/2024/march.html#google-cloud-vertex-ai-supported-as-model-provider-for-external-models)\\n+ [Access resources from serving endpoints using instance profiles is GA](https://docs.databricks.com/release-notes/product/2024/march.html#access-resources-from-serving-endpoints-using-instance-profiles-is-ga)\\n+ [Interactive notebook debugging](https://docs.databricks.com/release-notes/product/2024/march.html#interactive-notebook-debugging)\\n+ [Self-service sign-up for private exchange providers in Marketplace](https://docs.databricks.com/release-notes/product/2024/march.html#self-service-sign-up-for-private-exchange-providers-in-marketplace)\\n+ [Databricks Runtime 15.0 is GA](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-is-ga)\\n+ [Databricks Repos changed to Git folders](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-repos-changed-to-git-folders)\\n+ [Databricks Runtime 14.1 and 14.2 series support extended](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-141-and-142-series-support-extended)\\n+ [Databricks ODBC driver 2.8.0](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-odbc-driver-280)\\n+ [SQL warehouses for notebooks is GA](https://docs.databricks.com/release-notes/product/2024/march.html#sql-warehouses-for-notebooks-is-ga)\\n+ [Delegate the ability to view an object\\u2019s metadata in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/march.html#delegate-the-ability-to-view-an-objects-metadata-in-unity-catalog-public-preview)\\n+ [Databricks Runtime 15.0 (Beta)](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-beta)\\n+ [Databricks Runtime 14.0 series support ends](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-140-series-support-ends)\\n+ [New computation for sys.path and CWD in Repos](https://docs.databricks.com/release-notes/product/2024/march.html#new-computation-for-syspath-and-cwd-in-repos)\\n+ [Fe\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/release-notes/product/index.html\", \"score\": 2, \"chunk_id\": \"3c7d1aad122c45b35c16a852e6cc35b5\"}}]",
          false,
          null,
          "tool_outputs"
         ]
        ],
        [
         [
          [
           "You are a helpful AI assistant engaging in a conversation.",
           "system"
          ],
          [
           "what is lakehouse monitoring?",
           "user"
          ],
          [
           "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account, as well as track the performance of machine learning models and model-serving endpoints. This includes setting up alerts to notify you when certain metrics or statistics move out of a specified range, indicating potential issues such as data drift or the need for model retraining.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
           "assistant"
          ],
          [
           "How can I receive notifications for Databricks Lakehouse monitor alerts?",
           "user"
          ],
          [
           "To receive notifications for Databricks Lakehouse monitor alerts, you can set up alerts using Databricks SQL. Here's how you can do it:\n\n1. **Create a Databricks SQL Query**: First, create a SQL query on the monitor profile metrics table or drift metrics table. This query will define the conditions under which you want to be alerted.\n\n2. **Create a Databricks SQL Alert**: Once you have your query, create an alert for it. You can configure the alert to evaluate the query at a desired frequency.\n\n3. **Configure Notifications**: By default, email notifications are sent when an alert is triggered. However, you can also set up notifications to be sent to other applications such as Slack or PagerDuty using webhooks.\n\n4. **Quick Alert Creation from Dashboard**: You can also create an alert directly from the monitor dashboard:\n   - Find the chart for which you want to create an alert.\n   - Click the kebab menu (three vertical dots) in the upper-right corner of the chart and select \"View query.\"\n   - In the SQL editor, click the kebab menu above the editor window and select \"Create alert.\"\n   - Configure the alert and click \"Create alert.\"\n\nFor more detailed steps and options, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
           "assistant"
          ],
          [
           "How do I create an alert from the monitor dashboard?",
           "user"
          ]
         ],
         [
          "To create an alert from the monitor dashboard in Databricks, follow these steps:\n\n1. **Access the Monitor Dashboard**: Navigate to the monitor dashboard where you have the visualizations of your data metrics.\n\n2. **Select the Chart**: Identify the chart for which you want to create an alert. This chart should represent the data or metric you want to monitor.\n\n3. **Open the Chart Menu**: Click on the kebab menu (three vertical dots) located in the upper-right corner of the chart.\n\n4. **View the Query**: From the dropdown menu, select \"View query.\" This action will open the SQL editor with the query used to generate the chart.\n\n5. **Create the Alert**: In the SQL editor, locate the kebab menu above the editor window. Click on it and select \"Create alert.\"\n\n6. **Configure the Alert**: You will be prompted to configure the alert settings. This includes setting the conditions under which the alert should trigger, the frequency of evaluation, and the notification settings.\n\n7. **Finalize the Alert**: After configuring the alert, click \"Create alert\" to finalize and activate it.\n\nBy following these steps, you can easily set up alerts directly from the monitor dashboard, ensuring you receive timely notifications based on your data metrics.",
          "assistant"
         ],
         [
          "[{\"page_content\": \"# Introduction to Databricks Lakehouse Monitoring\\n### Monitor alerts\\n\\nPreview  \\nThis feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).  \\nThis page describes how to create a Databricks SQL alert based on a metric from a monitor metrics table. Some common uses for monitor alerts include:  \\n* Get notified when a statistic moves out of a certain range. For example, you want to receive a notification when the fraction of missing values exceeds a certain level.\\n* Get notified of a change in the data. The drift metrics table stores statistics that track changes in the data distribution.\\n* Get notified if data has drifted in comparison to the baseline table. You can set up an alert to investigate the data changes or, for `InferenceLog` analysis, to indicate that the model should be retrained.  \\nMonitor alerts are created and used the same way as other Databricks SQL alerts. You create a [Databricks SQL query](https://docs.databricks.com/sql/user/queries/index.html) on the monitor profile metrics table or drift metrics table. You then create a Databricks SQL alert for this query. You can configure the alert to evaluate the query at a desired frequency, and send a notification if the alert is triggered. By default, email notification is sent. You can also set up a webhook or send notifications to other applications such as Slack or Pagerduty.  \\nYou can also quickly create an alert from the [monitor dashboard](https://docs.databricks.com/lakehouse-monitoring/monitor-dashboard.html) as follows:  \\n1. On the dashboard, find the chart for which you want to create an alert.\\n2. Click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) in the upper-right corner of the chart and select **View query**. The SQL editor opens.\\n3. In the SQL editor, click ![Kebab menu](https://docs.databricks.com/_images/kebab-menu.png) above the editor window and select **Create alert**. The **New alert** dialog opens in a new tab.\\n4. Configure the alert and click **Create alert**.  \\nNote that if the query uses parameters, then the alert is based on the default values for these parameters. You should confirm that the default values reflect the intent of the alert.  \\nFor details, see [Databricks SQL alerts](https://docs.databricks.com/sql/user/alerts/index.html).\\n\\n\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html\", \"score\": 5, \"chunk_id\": \"ad46dd649af7864f934123978c11d492\"}}, {\"page_content\": \"# Databricks release notes\\n### Databricks platform release notes\\n\\n* [May 2024](https://docs.databricks.com/release-notes/product/2024/may.html)\\n+ [Compute plane outbound IP addresses must be added to a workspace IP allow list](https://docs.databricks.com/release-notes/product/2024/may.html#compute-plane-outbound-ip-addresses-must-be-added-to-a-workspace-ip-allow-list)\\n+ [OAuth is supported in Lakehouse Federation for Snowflake](https://docs.databricks.com/release-notes/product/2024/may.html#oauth-is-supported-in-lakehouse-federation-for-snowflake)\\n+ [Bulk move and delete workspace objects from the workspace browser](https://docs.databricks.com/release-notes/product/2024/may.html#bulk-move-and-delete-workspace-objects-from-the-workspace-browser)\\n+ [New compliance and security settings APIs (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#new-compliance-and-security-settings-apis-public-preview)\\n+ [Databricks Runtime 15.2 is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-is-ga)\\n+ [New Tableau connector for Delta Sharing](https://docs.databricks.com/release-notes/product/2024/may.html#new-tableau-connector-for-delta-sharing)\\n+ [New deep learning recommendation model examples](https://docs.databricks.com/release-notes/product/2024/may.html#new-deep-learning-recommendation-model-examples)\\n+ [Bind storage credentials and external locations to specific workspaces (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#bind-storage-credentials-and-external-locations-to-specific-workspaces-public-preview)\\n+ [Git folders are GA](https://docs.databricks.com/release-notes/product/2024/may.html#git-folders-are-ga)\\n+ [Pre-trained models in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#pre-trained-models-in-unity-catalog-public-preview)\\n+ [Databricks Vector Search is GA](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-vector-search-is-ga)\\n+ [The compliance security profile now supports AWS Graviton instance types](https://docs.databricks.com/release-notes/product/2024/may.html#the-compliance-security-profile-now-supports-aws-graviton-instance-types)\\n+ [Databricks Assistant autocomplete (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-assistant-autocomplete-public-preview)\\n+ [Meta Llama 3 support in Foundation Model Training](https://docs.databricks.com/release-notes/product/2024/may.html#meta-llama-3-support-in-foundation-model-training)\\n+ [New changes to Git folder UI](https://docs.databricks.com/release-notes/product/2024/may.html#new-changes-to-git-folder-ui)\\n+ [Compute now uses EBS GP3 volumes for autoscaling local storage](https://docs.databricks.com/release-notes/product/2024/may.html#compute-now-uses-ebs-gp3-volumes-for-autoscaling-local-storage)\\n+ [Unified Login now supported with AWS PrivateLink (Private Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#unified-login-now-supported-with-aws-privatelink-private-preview)\\n+ [Foundation Model Training (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#foundation-model-training-public-preview)\\n+ [Attribute tag values for Unity Catalog objects can now be 1000 characters long (Public Preview)](https://docs.databricks.com/release-notes/product/2024/may.html#attribute-tag-values-for-unity-catalog-objects-can-now-be-1000-characters-long-public-preview)\\n+ [New Previews page](https://docs.databricks.com/release-notes/product/2024/may.html#new-previews-page)\\n+ [New capabilities for Databricks Vector Search](https://docs.databricks.com/release-notes/product/2024/may.html#new-capabilities-for-databricks-vector-search)\\n+ [Credential passthrough and Hive metastore table access controls are deprecated](https://docs.databricks.com/release-notes/product/2024/may.html#credential-passthrough-and-hive-metastore-table-access-controls-are-deprecated)\\n+ [Databricks JDBC driver 2.6.38](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-jdbc-driver-2638)\\n+ [Databricks Runtime 15.2 (Beta)](https://docs.databricks.com/release-notes/product/2024/may.html#databricks-runtime-152-beta)\\n+ [Notebooks now detect and auto-complete column names for Spark Connect DataFrames](https://docs.databricks.com/release-notes/product/2024/may.html#notebooks-now-detect-and-auto-complete-column-names-for-spark-connect-dataframes)\\n* [April 2024](https://docs.databricks.com/release-notes/product/2024/april.html)\\n+ [Databricks Runtime 15.1 is GA](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-runtime-151-is-ga)\\n+ [Databricks Assistant: Threads & history](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-assistant-threads--history)\\n+ [Cancel pending serving endpoint updates in Model Serving](https://docs.databricks.com/release-notes/product/2024/april.html#cancel-pending-serving-endpoint-updates-in-model-serving)\\n+ [Data lineage now captures reads on tables with column masks and row-level security](https://docs.databricks.com/release-notes/product/2024/april.html#data-lineage-now-captures-reads-on-tables-with-column-masks-and-row-level-security)\\n+ [Meta Llama 3 is supported in Model Serving for AWS](https://docs.databricks.com/release-notes/product/2024/april.html#meta-llama-3-is-supported-in-model-serving-for-aws)\\n+ [Notebooks now automatically detect SQL](https://docs.databricks.com/release-notes/product/2024/april.html#notebooks-now-automatically-detect-sql)\\n+ [New columns added to the billable usage system table (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#new-columns-added-to-the-billable-usage-system-table-public-preview)\\n+ [Delta Sharing supports tables that use column mapping (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-sharing-supports-tables-that-use-column-mapping-public-preview)\\n+ [Get serving endpoint schemas (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#get-serving-endpoint-schemas-public-preview)\\n+ [Creation and installation of workspace libraries is no longer available](https://docs.databricks.com/release-notes/product/2024/april.html#creation-and-installation-of-workspace-libraries-is-no-longer-available)\\n+ [Jobs created through the UI are now queued by default](https://docs.databricks.com/release-notes/product/2024/april.html#jobs-created-through-the-ui-are-now-queued-by-default)\\n+ [Configuring access to resources from serving endpoints is GA](https://docs.databricks.com/release-notes/product/2024/april.html#configuring-access-to-resources-from-serving-endpoints-is-ga)\\n+ [Serverless compute for workflows is in public preview](https://docs.databricks.com/release-notes/product/2024/april.html#serverless-compute-for-workflows-is-in-public-preview)\\n+ [Lakehouse Federation supports foreign tables with case-sensitive identifiers](https://docs.databricks.com/release-notes/product/2024/april.html#lakehouse-federation-supports-foreign-tables-with-case-sensitive-identifiers)\\n+ [Compute cloning now clones any libraries installed on the original compute](https://docs.databricks.com/release-notes/product/2024/april.html#compute-cloning-now-clones-any-libraries-installed-on-the-original-compute)\\n+ [Route optimization is available for serving endpoints](https://docs.databricks.com/release-notes/product/2024/april.html#route-optimization-is-available-for-serving-endpoints)\\n+ [Delta Live Tables notebook developer experience improvements (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#delta-live-tables-notebook-developer-experience-improvements-public-preview)\\n+ [Databricks on AWS GovCloud (Public Preview)](https://docs.databricks.com/release-notes/product/2024/april.html#databricks-on-aws-govcloud-public-preview)\\n* [March 2024](https://docs.databricks.com/release-notes/product/2024/march.html)\\n+ [DBRX Base and DBRX Instruct are now available in Model Serving](https://docs.databricks.com/release-notes/product/2024/march.html#dbrx-base-and-dbrx-instruct-are-now-available-in-model-serving)\\n+ [Model Serving is HIPAA compliant in all regions](https://docs.databricks.com/release-notes/product/2024/march.html#model-serving-is-hipaa-compliant-in-all-regions)\\n+ [Provisioned throughput in Foundation Model APIs is GA and HIPAA compliant](https://docs.databricks.com/release-notes/product/2024/march.html#provisioned-throughput-in-foundation-model-apis-is-ga-and-hipaa-compliant)\\n+ [MLflow now enforces quota limits for experiments and runs](https://docs.databricks.com/release-notes/product/2024/march.html#mlflow-now-enforces-quota-limits-for-experiments-and-runs)\\n+ [The Jobs UI is updated to better manage jobs deployed by Databricks Asset Bundles](https://docs.databricks.com/release-notes/product/2024/march.html#the-jobs-ui-is-updated-to-better-manage-jobs-deployed-by-databricks-asset-bundles)\\n+ [Google Cloud Vertex AI supported as model provider for external models](https://docs.databricks.com/release-notes/product/2024/march.html#google-cloud-vertex-ai-supported-as-model-provider-for-external-models)\\n+ [Access resources from serving endpoints using instance profiles is GA](https://docs.databricks.com/release-notes/product/2024/march.html#access-resources-from-serving-endpoints-using-instance-profiles-is-ga)\\n+ [Interactive notebook debugging](https://docs.databricks.com/release-notes/product/2024/march.html#interactive-notebook-debugging)\\n+ [Self-service sign-up for private exchange providers in Marketplace](https://docs.databricks.com/release-notes/product/2024/march.html#self-service-sign-up-for-private-exchange-providers-in-marketplace)\\n+ [Databricks Runtime 15.0 is GA](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-is-ga)\\n+ [Databricks Repos changed to Git folders](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-repos-changed-to-git-folders)\\n+ [Databricks Runtime 14.1 and 14.2 series support extended](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-141-and-142-series-support-extended)\\n+ [Databricks ODBC driver 2.8.0](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-odbc-driver-280)\\n+ [SQL warehouses for notebooks is GA](https://docs.databricks.com/release-notes/product/2024/march.html#sql-warehouses-for-notebooks-is-ga)\\n+ [Delegate the ability to view an object\\u2019s metadata in Unity Catalog (Public Preview)](https://docs.databricks.com/release-notes/product/2024/march.html#delegate-the-ability-to-view-an-objects-metadata-in-unity-catalog-public-preview)\\n+ [Databricks Runtime 15.0 (Beta)](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-150-beta)\\n+ [Databricks Runtime 14.0 series support ends](https://docs.databricks.com/release-notes/product/2024/march.html#databricks-runtime-140-series-support-ends)\\n+ [New computation for sys.path and CWD in Repos](https://docs.databricks.com/release-notes/product/2024/march.html#new-computation-for-syspath-and-cwd-in-repos)\\n+ [Feature Serving is GA](https://docs.databricks.com/release-notes/product/2024/march.html#feature-serving-is-ga)\\n+ [Predictive optimization available in more regions](https://docs.databricks.com/release-notes/product/2024/march.html#predictive-optimization-available-in-more-regions)\\n* [February 2024](https://docs.databricks.com/release-notes/product/2024/february.html)\\n+ [Use Delta Live Tables in Feature Engineering (Public Preview)](https://docs.databricks.com/release-notes/product/2024/february.html#use-delta-live-tables-in-feature-engineering-public-preview)\\n+ [Restrict creating a personal access token for a service principal](https://docs.databricks.com/release-notes/product/2024/february.html#restrict-creating-a-personal-access-token-for-a-service-principal)\\n+ [Restrict changing a job owner and the run as setting](https://docs.databricks.com/release-notes/product/2024/february.html#restrict-changing-a-job-owner-and-the-run-as-setting)\\n+ [Automatic cluster update is enabled if the compliance security profile is enabled (GA)](https://docs.databricks.com/release-notes/product/2024/february.html#automatic-cluster-update-is-enabled-if-the-compliance-security-profile-is-enabled-ga-for-aws-cluster-update-is-changed-not-new-but-is-ga-now)\\n+ [Account admins can enable enhanced security and compliance features (Public Preview)](https://docs.databricks.com/release-notes/product/2024/february.html#account-admins-can-enable-enhanced-security-and-compliance-features-public-preview)\\n+ [Support for Cloudflare R2 storage to avoid cross-region egress fees (Public Preview)](https://docs.databricks.com/release-notes/product/2024/february.html#support-for-cloudflare-r2-storage-to-avoid-cross-region-egress-fees-public-preview)\\n+ [Notebooks for monitoring and managing Delta Sharing egress costs are now available](https://docs.databricks.com/release-notes/product/2024/february.html#notebooks-for-monitoring-and-managing-delta-sharing-egress-costs-are-now-available)\\n+ [Add data UI supports XML file format](https://docs.databricks.com/release-notes/product/2024/february.html#add-data-ui-supports-xml-file-format)\\n+ [Support for cloud storage firewall from serverless compute (Public Preview)](https://docs.databricks.com/release-notes/product/2024/february.html#support-for-cloud-storage-firewall-from-serverless-compute-public-preview)\\n+ [Use AI Functions to invoke a generative AI model from Foundation Model APIs](https://docs.databricks.com/release-notes/product/2024/february.html#use-ai-functions-to-invoke-a-generative-ai-model-from-foundation-model-apis)\\n+ [Unity Catalog volumes are GA](https://docs.databricks.com/release-notes/product/2024/february.html#unity-catalog-volumes-are-ga)\\n+ [Full-page AI-powered search](https://docs.databricks.com/release-notes/product/2024/february.html#full-page-ai-powered\", \"metadata\": {\"doc_uri\": \"https://docs.databricks.com/release-notes/product/index.html\", \"score\": 5, \"chunk_id\": \"3c7d1aad122c45b35c16a852e6cc35b5\"}}]",
          false,
          null,
          "tool_outputs"
         ]
        ],
        [
         [
          [
           "You are a helpful AI assistant engaging in a conversation.",
           "system"
          ],
          [
           "what is lakehouse monitoring?",
           "user"
          ]
         ],
         [
          "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
          "assistant"
         ],
         [
          null,
          true,
          "what is lakehouse monitoring?",
          "agent_response"
         ]
        ],
        [
         [
          [
           "You are a helpful AI assistant engaging in a conversation.",
           "system"
          ],
          [
           "what is lakehouse monitoring?",
           "user"
          ],
          [
           "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
           "assistant"
          ],
          [
           "What notification channels can you configure for monitor alerts in Databricks Lakehouse?",
           "user"
          ]
         ],
         [
          "In Databricks Lakehouse, you can configure monitor alerts to send notifications through various channels, including:\n\n1. Email\n2. Webhooks\n3. Applications like Slack\n4. PagerDuty\n\nThese channels allow you to receive alerts when certain metrics move out of a specified range or when there are significant changes in your data.",
          "assistant"
         ],
         [
          "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
          false,
          null,
          "agent_response"
         ]
        ],
        [
         [
          [
           "You are a helpful AI assistant engaging in a conversation.",
           "system"
          ],
          [
           "what is lakehouse monitoring?",
           "user"
          ],
          [
           "Lakehouse monitoring in Databricks refers to the capabilities that allow you to monitor the quality of data in all tables within your account and track the performance of machine learning models and model-serving endpoints. It includes features like monitor alerts, which notify you when certain metrics move out of a specified range or when data changes significantly. These alerts can be configured to send notifications via email, webhooks, or other applications like Slack or PagerDuty.\n\nFor more detailed information, you can refer to the [Databricks Lakehouse Monitoring documentation](https://docs.databricks.com/lakehouse-monitoring/monitor-alerts.html).",
           "assistant"
          ],
          [
           "What notification channels can you configure for monitor alerts in Databricks Lakehouse?",
           "user"
          ],
          [
           "In Databricks Lakehouse, you can configure monitor alerts to send notifications through various channels, including:\n\n1. Email\n2. Webhooks\n3. Applications like Slack\n4. PagerDuty\n\nThese channels allow you to receive alerts when certain metrics move out of a specified range or when there are significant changes in your data.",
           "assistant"
          ],
          [
           "What channels can you configure for Databricks Lakehouse monitor alerts?",
           "user"
          ]
         ],
         [
          "I couldn't find specific documentation on the notification channels for Databricks Lakehouse monitor alerts. However, typically, you can configure alerts to send notifications via email, webhooks, and applications like Slack and PagerDuty. If you need more detailed or specific information, I recommend checking the official Databricks documentation or contacting their support.",
          "assistant"
         ],
         [
          "In Databricks Lakehouse, you can configure monitor alerts to send notifications through various channels, including:\n\n1. Email\n2. Webhooks\n3. Applications like Slack\n4. PagerDuty\n\nThese channels allow you to receive alerts when certain metrics move out of a specified range or when there are significant changes in your data.",
          false,
          null,
          "agent_response"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "request",
         "type": "{\"containsNull\":true,\"elementType\":{\"fields\":[{\"metadata\":{},\"name\":\"content\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"role\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"},\"type\":\"array\"}"
        },
        {
         "metadata": "{}",
         "name": "response",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"content\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"role\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "metadata",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"context_used_to_generate_question\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"first_turn\",\"nullable\":true,\"type\":\"boolean\"},{\"metadata\":{},\"name\":\"seed_question\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"tag\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "synthetic_questions_df = pd.read_json(output_file, orient=\"records\", lines=True)\n",
    "display(synthetic_questions_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "conversation_simulator_demo_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
