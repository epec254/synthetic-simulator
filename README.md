# Synthetic Converastion Simulator

A Python utility for generating dynamic multi-turn conversations using [Agent Evaluation's Synthetic Generation API](https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html). This utility simulates user interactions with your Agent's model using Agent Evaluation's Synthetic Generation API to dynamically generate follow-up questions.

## Overview

This service is designed for:
- Generating synthetic evaluation data
- Testing agents in a multi-turn setting
- Simulating user interactions with AI agents

The service maintains conversation state and handles the interaction between:
- **Agent's Logged MLflow Model**: Provides AI-generated responses (hosted on Databricks)
- **Agent Evaluation Synthetic Data Ganeration API**: Generates contextually relevant follow-up questions

See [`example_output.jsonl`](example_output.jsonl) for sample conversation output generated by this service.

## Quickly Getting Started

### Using a Databricks Notebook

1. Clone the repository into your workspace:
    - New -> More -> Git Folder -> paste in `https://github.com/epec254/synthetic-simulator`
2. Open the Notebook [`conversation_simulator_demo_notebook.ipynb`](conversation_simulator_demo_notebook.ipynb) 
3. Connect to Serverless compute or DBR/MLR 15.4+ classic compute
4. Press Run All

### Using a local IDE

1. Clone the repository and install dependencies ([see Prerequisites](#prerequisites) for requirements):
```bash
git clone https://github.com/epec254/synthetic-simulator
poetry install
```

2. Set up your Databricks credentials ([see Environment Variables](#environment-variables) for details):
```bash
cp .env.sample .env
# Edit .env with your Databricks token and host
```

3. Run the example ([see Usage](#usage) for full documentation):
```bash
poetry run python conversation_simulator_demo.py
```

Alternatively, you can activate the poetry environment and run the example:
```bash
poetry shell
python conversation_simulator_demo.py
```

See the [Configuration](#configuration) section for detailed setup instructions.

## Prerequisites

Before you begin, ensure you have:
- Basic understanding of MLflow
- A Databricks workspace account
- If using a local IDE
    - Poetry (Python package manager)
    - Python 3.11 or higher installed
- If using a Databricks Notebook
    - Serverless compute or MLR/DBR 15.4+

### Installing Poetry

Follow the Poetry Installation [instructions](https://python-poetry.org/docs/#installing-with-pipx)

Verify the installation:
```bash
poetry --version
```



## Configuration: only required for using in a Local IDE

**Not required if running from a Databricks Notebook**

### 1. Databricks Setup

1. Log into your Databricks workspace
2. Generate a Personal Access Token:
   - Go to User Settings → Developer
   - Click "Manage Access Tokens" → "Generate New Token"
   - Save the token securely

### 2. Environment Variables

Copy the sample environment file:
```bash
cp .env.sample .env
```

Update `.env` with your values:
```bash
# Databricks Configuration
DATABRICKS_TOKEN=<your-databricks-token>    # The PAT token generated above
DATABRICKS_HOST=<workspace-url>             # e.g., https://your-org.cloud.databricks.com

# MLflow Configuration
MLFLOW_TRACKING_URI=databricks              # Keep as 'databricks' for Databricks integration
MLFLOW_EXPERIMENT_NAME=/Users/<email>/chat-service-experiment  # Your MLflow experiment path
```

> The MLflow experiment path is used to store / access the the logged Agent model and store the MLflow Traces generated during the simulation.

## Usage

The easiest way to get started is to use `conversation_simulator_demo.py` (from a Local IDE) or `conversation_simulator_demo_notebook.py` (from a Databricks Notebook) in the root directory. These files are the same, except one is designed as a Notebook and the other as a executable script.  This example shows how to:
1. Log your agent as an MLflow model
2. Create the required callables using provided utility functions
3. Set up and run the synthetic data generation

Additional example files in the root directory include:
- `fc_agent.py`: Example implementation of a function calling agent
- `model_utils.py`: Utility functions for model interaction

You can see example output in [`example_output.jsonl`](example_output.jsonl).

For a complete working example with error handling and additional features, see `conversation_simulator_demo.py`.

Here's a simplified version of the example:

```python
from simulator_service import SyntheticDataSimulatorService
import mlflow
from simulator_service.context_generators import get_all_tool_outputs_from_agent_trace
from simulator_service.synthetic_generation import generate_next_question_using_context_from_previous_turn
from model_utils import invoke_model_with_trace

# 1. Log your agent as an MLflow model
model_info = mlflow.pyfunc.log_model(
    python_model="path/to/agent.py",
    artifact_path="agent",
    # ... other configuration
)

# 2. Create a chat agent callable using the logged model
def chat_agent_callable(messages):
    loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)
    outputs, output_trace = invoke_model_with_trace(
        model=loaded_model,
        model_input={"messages": messages},
    )
    return {
        "outputs": outputs,
        "output_trace": output_trace,
    }

# 3. Initialize and run the chat service
chat_service = SyntheticDataSimulatorService(
    chat_agent_callable=chat_agent_callable,
    question_generator_callable=generate_next_question_using_context_from_previous_turn,
    get_context_from_chat_agent_response_for_next_turn_callable=get_all_tool_outputs_from_agent_trace,
    max_turns=5,
    seed_question="What is lakehouse monitoring?",
    output_file="conversation_output.jsonl", #where to save the resulting synthetic data
    agent_description="An AI assistant that helps with Databricks lakehouse monitoring" # helps tune the synthetic data generation to be aware of your agent's domain
)

simulator_service.start_conversation()
```

# API documentation
## Required Callable Signatures

The service requires three callable functions with specific signatures:

#### 1. chat_agent_callable

```python
def chat_agent_callable(messages: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    Function that calls the agent's MLflow model to get a response.
    
    Args:
        messages: List of message dictionaries with 'role' and 'content' keys
                 Example: [{"role": "user", "content": "What is lakehouse monitoring?"}]
    
    Returns:
        Dict containing at least:
            - outputs: The model's outputs
            - output_trace: The MLflow trace of the model's execution
    """
```

#### 2. question_generator_callable

> **Note:** You should not need to modify the default implentation of this callable in `simulator_service.synthetic_generation` - it's built to work with Databricks' Agent Evaluation Synthetic Generation API. However, you can replace it with your own function if you want to generate the next turn of conversation using a custom LLM call or different approach.

```python
def question_generator_callable(context: str, agent_description: str) -> List[Dict[str, str]]:
    """
    Function that generates the next question using Agent Evaluation API.
    
    Args:
        context: String containing context from the previous turn
        agent_description: Description of the agent's capabilities
    
    Returns:
        List of message dictionaries with 'role' and 'content' keys
        Example: [{"role": "user", "content": "Can you explain more about data quality monitoring?"}]
    """
```

#### 3. get_context_from_chat_agent_response_for_next_turn_callable

This callable extracts data from the agent's last response, which is then used as input to the Synthetic Generation API to generate the next conversation turn. 

We provide two example implementations in `simulator_service.context_generators`:
- `get_agent_response_from_trace`: Uses just the agent's final response text
- `get_all_tool_outputs_from_agent_trace`: Uses the outputs from all tools the agent called, providing richer context

You can implement your own version to customize how context is extracted and influence the synthetic conversation generation.

```python
def get_context_from_chat_agent_response_for_next_turn_callable(
    agent_response: Dict[str, Any]
) -> str:
    """
    Function that extracts context from the agent's response to use as input to the Synthetic API to generate the next user question to ask the agent..
    
    Args:
        agent_response: Dictionary containing the agent's response with outputs and trace
    
    Returns:
        String containing the extracted context for generating the next question
    
    Raises:
        EmptyContextError: If no context could be extracted from the response
    """
```


### SyntheticDataSimulatorService

Main class for generating synthetic conversations in `simulator_service.service`:

```python
SyntheticDataSimulatorService(
    chat_agent_callable: Callable,          # Function that calls the agent's MLflow model
    question_generator_callable: Callable,   # Function that generates questions using Agent Evaluation API
    get_context_from_chat_agent_response_for_next_turn_callable: Callable,  # Function that extracts context from agent response
    max_turns: int,                         # Maximum conversation turns
    seed_question: str,                     # Initial question to start with
    output_file: str = "conversation_history.jsonl",  # Where to save the conversation
    agent_description: Optional[str] = None,          # Description of the agent for question generation
    use_last_context_if_cannot_generate_context: bool = False,  # Fallback behavior for context extraction
    tag: Optional[str] = None               # Optional tag for conversation turns
)
```

## Model Utilities

> TODO: Add a version of `invoke_model_with_trace` that works with a Databricks Model Serving endpoint

The service provides utility functions in `model_utils.py` to help with model invocation and MLflow trace capture:

### invoke_model_with_trace

```python
def invoke_model_with_trace(
    model: mlflow.pyfunc.PyFuncModel,
    model_input: Any,
) -> Tuple[Any, Any]:
    """
    Invoke a model and capture its MLflow trace information.
    
    Args:
        model: The MLflow model to invoke (loaded via mlflow.pyfunc.load_model)
        model_input: Input to pass to the model (e.g., {"messages": [...]})
    
    Returns:
        Tuple of (model_output, mlflow_trace)
    """
```

### invoke_model_uri_with_trace

```python
def invoke_model_uri_with_trace(
    model_uri: str,
    model_input: Any,
) -> Tuple[Any, Any]:
    """
    Load a model from a URI and invoke it with trace capture.
    
    Args:
        model_uri: URI of the MLflow model (e.g., "models:/my-model/production")
        model_input: Input to pass to the model
    
    Returns:
        Tuple of (model_output, mlflow_trace)
    """
```

### load_and_invoke_model

```python
def load_and_invoke_model(
    model_info: Union[ModelInfo, str],
    model_input: Any,
) -> Tuple[Any, Any]:
    """
    Load a model from ModelInfo or URI and invoke it with trace capture.
    
    Args:
        model_info: Either a ModelInfo object from log_model() or a model URI string
        model_input: Input to pass to the model
    
    Returns:
        Tuple of (model_output, mlflow_trace)
    """
```

These utilities handle:
- Loading MLflow models
- Setting up the prediction context
- Capturing MLflow traces during model invocation

Example usage in your chat agent callable:

```python
from model_utils import invoke_model_with_trace

def chat_agent_callable(messages):
    # Load your model
    model = mlflow.pyfunc.load_model(model_uri)
    
    # Invoke with trace capture
    outputs, output_trace = invoke_model_with_trace(
        model=model,
        model_input={"messages": messages}
    )
    
    return {
        "outputs": outputs,
        "output_trace": output_trace  # Required for context extraction
    }
```

# Troubleshooting

Common issues and solutions:

1. **Poetry Installation Fails**
   - Ensure Python 3.11+ is installed: `python --version`
   - Try installing Poetry via pip: `pip install poetry`

2. **Databricks Connection Issues**
   - Verify your token is valid and not expired
   - Check if DATABRICKS_HOST includes the full URL with https://
   - Ensure your IP is allowlisted in Databricks workspace settings

3. **MLflow Errors**
   - Confirm MLFLOW_TRACKING_URI is set to "databricks"
   - Verify you have permission to create experiments in the specified path



For detailed API documentation, see the docstrings in the source code.

## Contributing

Contributions are welcome! Please submit a pull request with any improvements or bug fixes.

## License

Your use of this code is governed by the [Databricks License](LICENSE).
